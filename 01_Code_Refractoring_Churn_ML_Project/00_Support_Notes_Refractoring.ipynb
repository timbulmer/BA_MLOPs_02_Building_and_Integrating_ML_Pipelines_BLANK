{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f5e0fc8",
      "metadata": {},
      "source": [
        "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro_churn_prediction_pipeline",
      "metadata": {},
      "source": [
        "# End to End Customer Churn Prediction Pipeline\n",
        "\n",
        "This Jupyter Notebook provides a comprehensive demonstration of a Machine Learning pipeline for **Customer Churn Prediction**. It encompasses the entire lifecycle from raw data ingestion to model deployment and evaluation.\n",
        "\n",
        "The notebook showcases:\n",
        "\n",
        "* **Data Loading**: Functions for efficiently loading the raw customer churn dataset.\n",
        "* **Data Preprocessing & Transformation**: Steps for cleaning, handling missing values, encoding categorical features, scaling numerical features, and splitting data into training and testing sets.\n",
        "* **Model Definition & Training**: Implementation of a churn prediction model, including its training on the prepared data.\n",
        "* **Model Evaluation**: Calculation and reporting of key classification metrics to assess model performance.\n",
        "* **Model & Log Saving**: Persistence of the trained model and its performance metrics for future use and traceability.\n",
        "\n",
        "Initially, a single, comprehensive script is presented, followed by a demonstration of how its functionalities can be **refactored into modular components** (e.g., `config.py`, `data_loader.py`, `preprocessing.py`) for better organization, reusability, and maintainability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe0d2c9",
      "metadata": {},
      "source": [
        "Original code: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf5436f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Optional, Any, cast \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# ──────────────────────────────────────────────────\n",
        "# STATELESS HELPER FUNCTIONS\n",
        "# ──────────────────────────────────────────────────\n",
        "\n",
        "def load_churn_dataset(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads the Customer Churn dataset from a CSV file.\n",
        "    \n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: The raw churn dataset.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Dataset loaded from CSV: {filepath}\")\n",
        "        print(f\"Raw dataset: {len(df)} samples, {len(df.columns)} features\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load churn dataset: {e}\")\n",
        "\n",
        "def clean_churn_data(\n",
        "    df: pd.DataFrame, \n",
        "    target_column: str, \n",
        "    numeric_columns: list[str], \n",
        "    categorical_columns: list[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans the churn dataset by validating required columns and basic data type handling.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Raw dataset.\n",
        "        target_column (str): Target column that must be present.\n",
        "        numeric_columns (list[str]): Numeric feature columns.\n",
        "        categorical_columns (list[str]): Categorical feature columns.\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned dataset.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    print(f\"Initial data shape: {df_clean.shape}\")\n",
        "    \n",
        "    # Validate that required columns are present\n",
        "    all_required_columns = [target_column] + numeric_columns + categorical_columns\n",
        "    missing_columns = [col for col in all_required_columns if col not in df_clean.columns]\n",
        "    \n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns in dataset: {missing_columns}\")\n",
        "    \n",
        "    print(f\"All required columns found: {all_required_columns}\")\n",
        "    \n",
        "    # Keep only the required columns\n",
        "    df_clean = df_clean[all_required_columns].copy()\n",
        "    print(f\"Kept only required columns: {list(df_clean.columns)}\")\n",
        "    \n",
        "    print(f\"Missing values before cleaning:\\n{df_clean.isnull().sum()}\")\n",
        "    \n",
        "    # Convert TotalCharges to numeric (it might be stored as string)\n",
        "    if 'TotalCharges' in df_clean.columns:\n",
        "        df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
        "        print(f\"Converted TotalCharges to numeric. New missing values: {df_clean['TotalCharges'].isnull().sum()}\")\n",
        "    \n",
        "    # Clean target variable - standardize Yes/No to 1/0\n",
        "    if target_column in df_clean.columns:\n",
        "        df_clean['churn_binary'] = df_clean[target_column].map({'Yes': 1, 'No': 0})\n",
        "        print(f\"Target variable distribution:\\n{df_clean['churn_binary'].value_counts()}\")\n",
        "    \n",
        "    print(f\"Final data shape after cleaning: {df_clean.shape}\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "def build_preprocessing_pipeline(numeric_features: list[str], categorical_features: list[str]) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Builds a ColumnTransformer for preprocessing numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "        numeric_features (list[str]): list of numerical feature names.\n",
        "        categorical_features (list[str]): list of categorical feature names.\n",
        "\n",
        "    Returns:\n",
        "        ColumnTransformer: The configured ColumnTransformer.\n",
        "    \"\"\"\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),  # Handle potential NaNs\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle potential NaNs\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # sparse_output=False for easier handling\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop'  # Drop any columns not specified\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "def transform_features(\n",
        "    df: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    numeric_columns: list[str], \n",
        "    categorical_columns: list[str]\n",
        ") -> tuple[pd.DataFrame, ColumnTransformer, LabelEncoder]:\n",
        "    \"\"\"\n",
        "    Transforms features by encoding target variable and creating preprocessing pipeline.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Cleaned dataset.\n",
        "        target_column (str): Target column name.\n",
        "        numeric_columns (list[str]): Numeric feature columns.\n",
        "        categorical_columns (list[str]): Categorical feature columns.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, ColumnTransformer, LabelEncoder]: \n",
        "            - Dataset with encoded target\n",
        "            - Preprocessing pipeline for features\n",
        "            - Label encoder for target variable\n",
        "    \"\"\"\n",
        "    df_transformed = df.copy()\n",
        "    \n",
        "    # Encode target variable (churn_binary already created in cleaning)\n",
        "    if 'churn_binary' in df_transformed.columns:\n",
        "        # For churn, we don't need label encoding since it's already 0/1\n",
        "        # But we'll create a dummy encoder for consistency\n",
        "        label_encoder = LabelEncoder()\n",
        "        df_transformed['target_encoded'] = df_transformed['churn_binary']\n",
        "        \n",
        "        # Store mapping information\n",
        "        churn_mapping = {0: 'No Churn', 1: 'Churn'}\n",
        "        df_transformed.attrs['target_mapping'] = churn_mapping\n",
        "        df_transformed.attrs['target_names'] = ['No Churn', 'Churn']\n",
        "        \n",
        "        print(f\"Target encoding - Churn mapping: {churn_mapping}\")\n",
        "    else:\n",
        "        raise ValueError(\"Churn binary column not found in dataset\")\n",
        "    \n",
        "    # Filter available features\n",
        "    available_numeric = [col for col in numeric_columns if col in df_transformed.columns]\n",
        "    available_categorical = [col for col in categorical_columns if col in df_transformed.columns]\n",
        "    \n",
        "    print(f\"Available numeric features: {available_numeric}\")\n",
        "    print(f\"Available categorical features: {available_categorical}\")\n",
        "    \n",
        "    # Build preprocessing pipeline\n",
        "    preprocessor = build_preprocessing_pipeline(available_numeric, available_categorical)\n",
        "    \n",
        "    # Store feature information for later use\n",
        "    all_features = available_numeric + available_categorical\n",
        "    df_transformed.attrs['feature_columns'] = all_features\n",
        "    df_transformed.attrs['numeric_features'] = available_numeric\n",
        "    df_transformed.attrs['categorical_features'] = available_categorical\n",
        "    df_transformed.attrs['preprocessor'] = preprocessor\n",
        "    \n",
        "    print(f\"Features for modeling: {all_features}\")\n",
        "    print(f\"Preprocessing pipeline created with {len(available_numeric)} numeric and {len(available_categorical)} categorical features\")\n",
        "    \n",
        "    return df_transformed, preprocessor, label_encoder\n",
        "\n",
        "def split_features_and_target(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into features and target variable.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Transformed dataset.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: Features (X) and target (y).\n",
        "    \"\"\"\n",
        "    # Get feature columns from transformation step\n",
        "    feature_columns = df.attrs.get('feature_columns', [])\n",
        "    \n",
        "    if not feature_columns:\n",
        "        raise ValueError(\"No feature columns found in dataset attributes\")\n",
        "    \n",
        "    # Ensure all required features are present\n",
        "    missing_features = [f for f in feature_columns if f not in df.columns]\n",
        "    if missing_features:\n",
        "        raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "    \n",
        "    X = df[feature_columns].copy()\n",
        "    y = df['target_encoded'].copy()\n",
        "    \n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Features used: {list(X.columns)}\")\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def stratified_split(\n",
        "    X: pd.DataFrame, \n",
        "    y: pd.Series, \n",
        "    test_size: float = 0.25, \n",
        "    seed: int = 42\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets with stratification.\n",
        "    \n",
        "    Args:\n",
        "        X (pd.DataFrame): Features.\n",
        "        y (pd.Series): Target.\n",
        "        test_size (float): Proportion of test data.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        \n",
        "    Returns:\n",
        "        Tuple: Split data - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    # Cast to tuple to match the type hint\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def compute_classification_metrics(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray, \n",
        "    target_names: Optional[list[str]] = None\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes classification metrics for binary classification.\n",
        "    \n",
        "    Args:\n",
        "        y_true (np.ndarray): True labels.\n",
        "        y_pred (np.ndarray): Predicted labels.\n",
        "        target_names (Optional[list[str]]): Names of target classes.\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary containing all computed metrics.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, average='binary'),\n",
        "        \"recall\": recall_score(y_true, y_pred, average='binary'),\n",
        "        \"f1_score\": f1_score(y_true, y_pred, average='binary'),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
        "        \"classification_report\": classification_report(\n",
        "            y_true, y_pred, \n",
        "            target_names=target_names,\n",
        "            output_dict=False\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def report_classification_metrics(metrics: dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Prints formatted classification metrics.\n",
        "    \n",
        "    Args:\n",
        "        metrics (Dict[str, Any]): Model evaluation metrics.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    output_lines.append(\"\\n\" + \"=\"*40)\n",
        "    output_lines.append(\"CLASSIFICATION METRICS\")\n",
        "    output_lines.append(\"=\"*40)\n",
        "    \n",
        "    # Check for key existence to prevent KeyErrors if metrics dict is incomplete\n",
        "    output_lines.append(f\"Accuracy : {metrics.get('accuracy', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"Precision: {metrics.get('precision', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"Recall   : {metrics.get('recall', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"F1-Score : {metrics.get('f1_score', float('nan')):.4f}\")\n",
        "    \n",
        "    output_lines.append(f\"\\nConfusion Matrix:\")\n",
        "    confusion_matrix_data = metrics.get('confusion_matrix')\n",
        "    if isinstance(confusion_matrix_data, list):\n",
        "        for row in confusion_matrix_data:\n",
        "            output_lines.append(f\"   {row}\")\n",
        "    elif confusion_matrix_data is not None:\n",
        "        output_lines.append(f\"   Unexpected format for confusion matrix: {confusion_matrix_data}\")\n",
        "    else:\n",
        "        output_lines.append(f\"   Confusion matrix data not available.\")\n",
        "        \n",
        "    print(\"\\n\".join(output_lines))\n",
        "\n",
        "# ──────────────────────────────────────────────────\n",
        "# STATEFUL MODEL CLASS\n",
        "# ──────────────────────────────────────────────────\n",
        "\n",
        "class ChurnPredictionModel:\n",
        "    \"\"\"\n",
        "    Customer churn prediction model using configurable classifier with preprocessing pipeline.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        classifier=None,\n",
        "        preprocessor=None,\n",
        "        random_state: int = 42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the churn prediction pipeline.\n",
        "        \n",
        "        Args:\n",
        "            classifier: Scikit-learn classifier instance. If None, uses LogisticRegression.\n",
        "            preprocessor: Scikit-learn preprocessing pipeline. If None, uses StandardScaler only.\n",
        "            random_state (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        \n",
        "        # Use provided classifier or default to LogisticRegression\n",
        "        if classifier is None:\n",
        "            self.classifier = LogisticRegression(\n",
        "                random_state=self.random_state,\n",
        "                max_iter=1000,\n",
        "                class_weight='balanced'  # Handle potential class imbalance\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = classifier\n",
        "        \n",
        "        # Use provided preprocessor or create simple scaling pipeline\n",
        "        if preprocessor is None:\n",
        "            self.pipe = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('classifier', self.classifier)\n",
        "            ])\n",
        "        else:\n",
        "            self.pipe = Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('classifier', self.classifier)\n",
        "            ])\n",
        "    \n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ChurnPredictionModel':\n",
        "        \"\"\"\n",
        "        Fit the model to training data.\n",
        "        \n",
        "        Args:\n",
        "            X (pd.DataFrame): Training features.\n",
        "            y (pd.Series): Training target.\n",
        "            \n",
        "        Returns:\n",
        "            ChurnPredictionModel: Self for method chaining.\n",
        "        \"\"\"\n",
        "        print(f\"Training model with {len(X)} samples and {len(X.columns)} features...\")\n",
        "        \n",
        "        # Fit the pipeline\n",
        "        self.pipe.fit(X, y)\n",
        "        \n",
        "        print(f\"Model trained successfully!\")\n",
        "        print(f\"Features used: {list(X.columns)}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions on input data.\n",
        "        \n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "        return cast(np.ndarray, self.pipe.predict(X))\n",
        "    \n",
        "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict class probabilities.\n",
        "        \n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "            \n",
        "        Returns:\n",
        "            np.ndarray: Predicted class probabilities.\n",
        "        \"\"\"\n",
        "        return cast(np.ndarray, self.pipe.predict_proba(X))\n",
        "    \n",
        "    def save(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained model to a file.\n",
        "        \n",
        "        Args:\n",
        "            filepath (str): Full path to save the model.\n",
        "        \"\"\"\n",
        "        directory = os.path.dirname(filepath)\n",
        "        if directory and not os.path.exists(directory):\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "        \n",
        "        joblib.dump(self.pipe, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "    \n",
        "    def save_run_log(self, directory: str, metrics: dict[str, Any], dataset_info: dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Save model configuration and performance metrics to a JSON file.\n",
        "        \n",
        "        Args:\n",
        "            directory (str): Directory where the JSON file will be stored.\n",
        "            metrics (Dict[str, Any]): Evaluation metrics to save.\n",
        "            dataset_info (Dict[str, Any]): Information about the dataset used.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "        \n",
        "        run_info = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"model_class\": \"ChurnPredictionModel\",\n",
        "            \"classifier\": str(type(self.classifier).__name__),\n",
        "            \"dataset\": \"Customer Churn\",\n",
        "            \"dataset_info\": dataset_info,\n",
        "            \"parameters\": {\n",
        "                \"random_state\": self.random_state,\n",
        "                \"classifier_params\": self.classifier.get_params()\n",
        "            },\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "        \n",
        "        log_file = os.path.join(directory, \"churn_model_run_log.json\")\n",
        "        \n",
        "        # Load existing logs or create new list\n",
        "        if os.path.exists(log_file):\n",
        "            try:\n",
        "                with open(log_file, \"r\") as f:\n",
        "                    logs = json.load(f)\n",
        "            except (json.JSONDecodeError, FileNotFoundError):\n",
        "                logs = []\n",
        "        else:\n",
        "            logs = []\n",
        "        \n",
        "        logs.append(run_info)\n",
        "        \n",
        "        # Save updated logs\n",
        "        with open(log_file, \"w\") as f:\n",
        "            json.dump(logs, f, indent=4, default=str)\n",
        "        \n",
        "        print(f\"Run log saved to {log_file}\")\n",
        "\n",
        "# ──────────────────────────────────────────────────\n",
        "# MAIN ORCHESTRATOR\n",
        "# ──────────────────────────────────────────────────\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the entire ML pipeline:\n",
        "    1. Load and prepare data\n",
        "    2. Split data\n",
        "    3. Train model\n",
        "    4. Evaluate performance\n",
        "    5. Save model and logs\n",
        "    \"\"\"\n",
        "    print(\"Starting Customer Churn Prediction Pipeline...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        # Define column structure based on the churn dataset\n",
        "        target_column = 'Churn'  # Target variable\n",
        "        numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "        categorical_columns = [\n",
        "            'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "            'PhoneService', 'MultipleLines', 'InternetService',\n",
        "            'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "            'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "            'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "        ]\n",
        "        \n",
        "        # Construct the absolute path to the data file (reverted to original path)\n",
        "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "        data_file_path = os.path.join(script_dir, \"data\", \"WA_Fn-UseC_-Telco-Customer-Churn.csv\") # Path restored\n",
        "\n",
        "        # Load and prepare data\n",
        "        print(\"\\n1. Loading dataset...\")\n",
        "        df_raw = load_churn_dataset(data_file_path)\n",
        "\n",
        "        \n",
        "        print(\"\\n2. Cleaning data...\")\n",
        "        df_clean = clean_churn_data(df_raw, target_column, numeric_columns, categorical_columns)\n",
        "        \n",
        "        print(\"\\n3. Transforming features...\")\n",
        "        df_transformed, preprocessor, label_encoder = transform_features(df_clean, target_column, numeric_columns, categorical_columns)\n",
        "        \n",
        "        # Extract dataset information\n",
        "        dataset_info = {\n",
        "            \"total_samples\": len(df_transformed),\n",
        "            \"n_features\": len(df_transformed.attrs.get('feature_columns', [])),\n",
        "            \"target_mapping\": df_transformed.attrs.get('target_mapping', {}),\n",
        "            \"target_names\": df_transformed.attrs.get('target_names', []),\n",
        "            \"churn_distribution\": df_transformed['target_encoded'].value_counts().to_dict(),\n",
        "            \"feature_columns\": df_transformed.attrs.get('feature_columns', []),\n",
        "            \"numeric_features\": df_transformed.attrs.get('numeric_features', []),\n",
        "            \"categorical_features\": df_transformed.attrs.get('categorical_features', [])\n",
        "        }\n",
        "        \n",
        "        # Split features and target\n",
        "        print(\"\\n4. Preparing features and target...\")\n",
        "        X, y = split_features_and_target(df_transformed)\n",
        "        \n",
        "        # Split into train/test\n",
        "        print(\"\\n5. Splitting data...\")\n",
        "        X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=0.25, seed=42)\n",
        "        print(f\"Training set: {len(X_train)} samples\")\n",
        "        print(f\"Test set: {len(X_test)} samples\")\n",
        "        print(f\"Train churn rate: {y_train.mean():.3f}\")\n",
        "        print(f\"Test churn rate: {y_test.mean():.3f}\")\n",
        "        \n",
        "        # Initialize and train model\n",
        "        print(\"\\n6. Training model...\")\n",
        "        model = ChurnPredictionModel(preprocessor=preprocessor, random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        print(\"\\n7. Making predictions...\")\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Evaluate model\n",
        "        print(\"\\n8. Evaluating model...\")\n",
        "        metrics = compute_classification_metrics(\n",
        "            y_test.to_numpy(),  # Use .to_numpy() for explicit np.ndarray conversion\n",
        "            y_pred, \n",
        "            target_names=dataset_info['target_names']\n",
        "        )\n",
        "        \n",
        "        # Report results\n",
        "        report_classification_metrics(metrics)\n",
        "        \n",
        "        # Save model and logs\n",
        "        print(\"\\n9. Saving model and logs...\")\n",
        "        # Define the name of the directory for saved models\n",
        "        saved_models_dirname = \"saved_models\"\n",
        "        # Create the full path for the model directory relative to the script's location\n",
        "        model_dir_path = os.path.join(script_dir, saved_models_dirname)\n",
        "        \n",
        "        # Create the full path for the model file\n",
        "        model_file_path = os.path.join(model_dir_path, \"churn_prediction_model_v1.joblib\")\n",
        "        \n",
        "        model.save(model_file_path) # Pass the full path to the model file\n",
        "        model.save_run_log(model_dir_path, metrics, dataset_info) # Pass the full path to the directory for logs\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        print(f\"Model accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"Model saved to: {model_file_path}\") # Updated to show the full path\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Pipeline failed with exception: {e}\")\n",
        "        raise\n",
        "\n",
        "# ──────────────────────────────────────────────────\n",
        "# ENTRY POINT\n",
        "# ──────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3231eca6",
      "metadata": {},
      "source": [
        "## Refractoring Process: \n",
        "\n",
        "### config.py\n",
        "\n",
        "Central definition of parameters and paths.\n",
        "\n",
        "This script stores configuration variables and constants used across the ML project. It includes definitions for column names, test sizes, random states, and file names for models and logs, and paths to datasets.\n",
        "\n",
        "* **Column Definitions**: Defines `TARGET_COLUMN` for the target variable 'Churn', and lists `NUMERIC_COLUMNS` and `CATEGORICAL_COLUMNS` for the churn dataset.\n",
        "* **Test and Random State Constants**: Specifies `TEST_SIZE` as 0.25 and `RANDOM_STATE` as 42 for reproducibility.\n",
        "* **File and Directory Names**: Defines filenames for the `MODEL_FILENAME` (\"churn_prediction_model_v1.joblib\") and `LOG_FILENAME` (\"churn_model_run_log.json\"), as well as the `MODEL_STORE_DIR` (\"model_store\").\n",
        "* **Data Paths**: Sets up directory names (`DATA_DIR_NAME`, `RAW_DATA_DIR_NAME`) and the `DATASET_FILENAME` (\"WA_Fn-UseC_-Telco-Customer-Churn.csv\") for dataset locations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f78e8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "config.py\n",
        "\n",
        "This script stores configuration variables and constants used across the ML project.\n",
        "It includes definitions for column names, test sizes, random states, and file names\n",
        "for models and logs, and paths to datasets.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "# Define column structure based on the churn dataset\n",
        "TARGET_COLUMN = 'Churn'  # Target variable\n",
        "NUMERIC_COLUMNS = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "CATEGORICAL_COLUMNS = [\n",
        "    'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "    'PhoneService', 'MultipleLines', 'InternetService',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "    'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
        "    'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "]\n",
        "\n",
        "\n",
        "TEST_SIZE = 0.25\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Define file and directory names\n",
        "MODEL_FILENAME = \"churn_prediction_model_v1.joblib\"\n",
        "LOG_FILENAME = \"churn_model_run_log.json\"\n",
        "MODEL_STORE_DIR = \"model_store\"\n",
        "\n",
        "# Define data paths (relative to the project root)\n",
        "DATA_DIR_NAME = \"data\"\n",
        "RAW_DATA_DIR_NAME = \"raw\"\n",
        "DATASET_FILENAME = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc83460",
      "metadata": {},
      "source": [
        "### data_loader.py\n",
        "\n",
        "Reading and basic cleaning of raw data. Initial splitting(e.g., sampling on large datasets).\n",
        "\n",
        "This script is responsible for loading the raw customer churn dataset. It contains functions to read the data from a specified file path.\n",
        "\n",
        "* **`load_churn_dataset(filepath: str) -> pd.DataFrame`**: This function loads the Customer Churn dataset from a specified CSV `filepath`.\n",
        "    * **Functionality**: It uses `pandas.read_csv` to load the data.\n",
        "    * **Output**: Prints information about the loaded dataset including shape, columns, and missing values. Returns a pandas DataFrame.\n",
        "    * **Error Handling**: Includes a `try-except` block to catch any exceptions during file loading and raises a `RuntimeError`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c00387",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "data_loader.py\n",
        "\n",
        "This script is responsible for loading the raw customer churn dataset.\n",
        "It contains functions to read the data from a specified file path.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming interfaces.py exists and defines DataLoader\n",
        "# from src.interfaces import DataLoader # Not needed for static type checking in runtime if not inheriting\n",
        "\n",
        "def load_churn_dataset(filepath: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads the Customer Churn dataset from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The raw churn dataset.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Dataset loaded from CSV: {filepath}\")\n",
        "        print(f\"Raw dataset: {len(df)} samples, {len(df.columns)} features\")\n",
        "        print(f\"Available columns: {list(df.columns)}\")\n",
        "        print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load churn dataset: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ca564c",
      "metadata": {},
      "source": [
        "### preprocessing.py\n",
        "\n",
        "This script contains functions for data cleaning and preprocessing. It handles tasks such as converting data types, encoding categorical variables, scaling numerical features, and splitting the data into training and testing sets.\n",
        "\n",
        "* **`clean_churn_data(...)` Function**:\n",
        "    * **Purpose**: Cleans the raw churn dataset, ensuring required columns are present and handling basic data type conversions.\n",
        "    * **Functionality**: Validates column presence, converts `TotalCharges` to numeric (handling coerce errors), and converts the target `Yes`/`No` to binary `1`/`0`. Selects only relevant columns for the output.\n",
        "    * **Inputs**: Raw DataFrame, target column name, lists of numeric and categorical column names.\n",
        "    * **Outputs**: Cleaned DataFrame.\n",
        "    * **Error Handling**: Raises `ValueError` if required columns are missing.\n",
        "\n",
        "* **`build_preprocessing_pipeline(...)` Function**:\n",
        "    * **Purpose**: Constructs a `ColumnTransformer` for comprehensive preprocessing of numerical and categorical features.\n",
        "    * **Functionality**: Defines pipelines for numeric (median imputation, standard scaling) and categorical (most frequent imputation, one-hot encoding) transformations. Combines these using `ColumnTransformer`.\n",
        "    * **Inputs**: Lists of numerical and categorical feature names.\n",
        "    * **Outputs**: Configured `ColumnTransformer`.\n",
        "\n",
        "* **`transform_features(...)` Function**:\n",
        "    * **Purpose**: Encodes the target variable and applies/fits the preprocessing pipeline to the features.\n",
        "    * **Functionality**: Ensures the target variable is encoded (assumes `churn_binary` is already present) and stores target mapping. Builds and fits the `ColumnTransformer` on the relevant features. Stores feature information in DataFrame attributes.\n",
        "    * **Inputs**: Cleaned DataFrame, target column name, lists of numeric and categorical column names.\n",
        "    * **Outputs**: Tuple containing the DataFrame with encoded target and the fitted `ColumnTransformer`.\n",
        "    * **Error Handling**: Raises `ValueError` if `churn_binary` column is not found.\n",
        "\n",
        "* **`split_features_and_target(...)` Function**:\n",
        "    * **Purpose**: Separates the DataFrame into feature (X) and target (y) DataFrames/Series.\n",
        "    * **Functionality**: Retrieves feature columns from DataFrame attributes and ensures they are present.\n",
        "    * **Inputs**: Transformed DataFrame.\n",
        "    * **Outputs**: Tuple of features `X` (DataFrame) and target `y` (Series).\n",
        "    * **Error Handling**: Raises `ValueError` if feature columns are not found in attributes or if any required features are missing.\n",
        "\n",
        "* **`stratified_split(...)` Function**:\n",
        "    * **Purpose**: Splits data into training and testing sets while preserving the proportion of the target variable (stratification).\n",
        "    * **Functionality**: Uses `sklearn.model_selection.train_test_split` with `stratify=y`.\n",
        "    * **Inputs**: Features `X`, target `y`, test size, and random seed.\n",
        "    * **Outputs**: Tuple of `X_train`, `X_test`, `y_train`, `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c0425b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "preprocessing.py\n",
        "\n",
        "This script contains functions for data cleaning and preprocessing.\n",
        "It handles tasks such as converting data types, encoding categorical variables,\n",
        "scaling numerical features, and splitting the data into training and testing sets.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "def clean_churn_data(\n",
        "    df: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    numeric_columns: list[str],\n",
        "    categorical_columns: list[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Cleans the churn dataset by validating required columns and basic data type handling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Raw dataset.\n",
        "        target_column (str): Target column that must be present.\n",
        "        numeric_columns (list[str]): Numeric feature columns.\n",
        "        categorical_columns (list[str]): Categorical feature columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned dataset.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    print(f\"Initial data shape: {df_clean.shape}\")\n",
        "\n",
        "    # Validate that required columns are present\n",
        "    all_required_columns = [target_column] + numeric_columns + categorical_columns\n",
        "    missing_columns = [col for col in all_required_columns if col not in df_clean.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing required columns in dataset: {missing_columns}\")\n",
        "\n",
        "    print(f\"All required columns found: {all_required_columns}\")\n",
        "\n",
        "    # Keep only the required columns\n",
        "    df_clean = df_clean[all_required_columns].copy()\n",
        "    print(f\"Kept only required columns: {list(df_clean.columns)}\")\n",
        "\n",
        "    print(f\"Missing values before cleaning:\\n{df_clean.isnull().sum()}\")\n",
        "\n",
        "    # Convert TotalCharges to numeric (it might be stored as string)\n",
        "    if 'TotalCharges' in df_clean.columns:\n",
        "        df_clean['TotalCharges'] = pd.to_numeric(df_clean['TotalCharges'], errors='coerce')\n",
        "        print(f\"Converted TotalCharges to numeric. New missing values: {df_clean['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "    # Clean target variable - standardize Yes/No to 1/0\n",
        "    if target_column in df_clean.columns:\n",
        "        df_clean['churn_binary'] = df_clean[target_column].map({'Yes': 1, 'No': 0})\n",
        "        print(f\"Target variable distribution:\\n{df_clean['churn_binary'].value_counts()}\")\n",
        "\n",
        "    # Define the set of columns that should be in the final output DataFrame\n",
        "    final_output_columns = numeric_columns + categorical_columns + ['churn_binary']\n",
        "\n",
        "    # Select only these columns\n",
        "    # Use .copy() to ensure this is a new DataFrame and avoid potential warnings later\n",
        "    df_clean = df_clean[final_output_columns].copy()\n",
        "    \n",
        "    print(f\"Final columns in returned DataFrame: {list(df_clean.columns)}\")\n",
        "\n",
        "    print(f\"Final data shape after cleaning: {df_clean.shape}\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def build_preprocessing_pipeline(numeric_features: list[str], categorical_features: list[str]) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Builds a ColumnTransformer for preprocessing numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "        numeric_features (list[str]): list of numerical feature names.\n",
        "        categorical_features (list[str]): list of categorical feature names.\n",
        "\n",
        "    Returns:\n",
        "        ColumnTransformer: The configured ColumnTransformer.\n",
        "    \"\"\"\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "def transform_features(\n",
        "    df: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    numeric_columns: list[str],\n",
        "    categorical_columns: list[str]\n",
        ") -> tuple[pd.DataFrame, ColumnTransformer]:\n",
        "    \n",
        "    #tuple[pd.DataFrame, ColumnTransformer, LabelEncoder]\n",
        "    \"\"\"\n",
        "    Transforms features by encoding target variable and creating preprocessing pipeline.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Cleaned dataset.\n",
        "        target_column (str): Target column name.\n",
        "        numeric_columns (list[str]): Numeric feature columns.\n",
        "        categorical_columns (list[str]): Categorical feature columns.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, ColumnTransformer, LabelEncoder]:\n",
        "            - Dataset with encoded target\n",
        "            - Preprocessing pipeline for features\n",
        "            - Label encoder for target variable\n",
        "    \"\"\"\n",
        "    df_transformed = df.copy()\n",
        "\n",
        "    # Encode target variable (churn_binary already created in cleaning)\n",
        "    if 'churn_binary' in df_transformed.columns:\n",
        "        # For churn, we don't need label encoding since it's already 0/1\n",
        "        # But we'll create a dummy encoder for consistency\n",
        "\n",
        "        #label_encoder = LabelEncoder()\n",
        "        df_transformed['target_encoded'] = df_transformed['churn_binary']\n",
        "\n",
        "        # Store mapping information\n",
        "        churn_mapping = {0: 'No Churn', 1: 'Churn'}\n",
        "        df_transformed.attrs['target_mapping'] = churn_mapping\n",
        "        df_transformed.attrs['target_names'] = ['No Churn', 'Churn']\n",
        "\n",
        "        print(f\"Target encoding - Churn mapping: {churn_mapping}\")\n",
        "    else:\n",
        "        raise ValueError(\"Churn binary column not found in dataset\")\n",
        "\n",
        "    # Filter available features\n",
        "    available_numeric = [col for col in numeric_columns if col in df_transformed.columns]\n",
        "    available_categorical = [col for col in categorical_columns if col in df_transformed.columns]\n",
        "\n",
        "    print(f\"Available numeric features: {available_numeric}\")\n",
        "    print(f\"Available categorical features: {available_categorical}\")\n",
        "\n",
        "    # Build preprocessing pipeline\n",
        "    preprocessor = build_preprocessing_pipeline(available_numeric, available_categorical)\n",
        "\n",
        "    # Fit the preprocessor on the feature data\n",
        "    features_to_fit = df_transformed[available_numeric + available_categorical]\n",
        "    if not features_to_fit.empty: # Ensure there are features to fit\n",
        "        preprocessor.fit(features_to_fit)\n",
        "\n",
        "    # Store feature information for later use\n",
        "    all_features = available_numeric + available_categorical\n",
        "    df_transformed.attrs['feature_columns'] = all_features\n",
        "    df_transformed.attrs['numeric_features'] = available_numeric\n",
        "    df_transformed.attrs['categorical_features'] = available_categorical\n",
        "    df_transformed.attrs['preprocessor'] = preprocessor\n",
        "\n",
        "    print(f\"Features for modeling: {all_features}\")\n",
        "    print(f\"Preprocessing pipeline created with {len(available_numeric)} numeric and {len(available_categorical)} categorical features\")\n",
        "\n",
        "    #return df_transformed, preprocessor, label_encoder\n",
        "    #return df_transformed, preprocessor, label_encoder\n",
        "    return df_transformed, preprocessor\n",
        "\n",
        "def split_features_and_target(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into features and target variable.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Transformed dataset.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: Features (X) and target (y).\n",
        "    \"\"\"\n",
        "    # Get feature columns from transformation step\n",
        "    feature_columns = df.attrs.get('feature_columns', [])\n",
        "\n",
        "    if not feature_columns:\n",
        "        raise ValueError(\"No feature columns found in dataset attributes\")\n",
        "\n",
        "    # Ensure all required features are present\n",
        "    missing_features = [f for f in feature_columns if f not in df.columns]\n",
        "    if missing_features:\n",
        "        raise ValueError(f\"Missing required features: {missing_features}\")\n",
        "\n",
        "    X = df[feature_columns].copy()\n",
        "    y = df['target_encoded'].copy()\n",
        "\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Features used: {list(X.columns)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def stratified_split(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    test_size: float = 0.25,\n",
        "    seed: int = 42\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets with stratification.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Features.\n",
        "        y (pd.Series): Target.\n",
        "        test_size (float): Proportion of test data.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Split data - X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
        "    )\n",
        "    # Cast to tuple to match the type hint\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3041c5a",
      "metadata": {},
      "source": [
        "### model.py\n",
        "\n",
        "Model training, inference, and saving logic.\n",
        "\n",
        "This script defines the `ChurnPredictionModel` class, which encapsulates the machine learning model (e.g., Logistic Regression) and its associated preprocessing pipeline. It also includes functions for computing and reporting classification metrics, and for logging model run details.\n",
        "\n",
        "* **`ChurnPredictionModel` Class**:\n",
        "    * **`__init__(self, classifier=None, preprocessor=None, random_state: int = 42)`**: Initializes the model pipeline. If no classifier or preprocessor is provided, it defaults to `LogisticRegression` and `StandardScaler`, respectively.\n",
        "    * **`fit(self, X: pd.DataFrame, y: pd.Series) -> 'ChurnPredictionModel'`**: Fits the machine learning pipeline to the training data.\n",
        "    * **`predict(self, X: pd.DataFrame) -> np.ndarray`**: Makes class label predictions on new data.\n",
        "    * **`predict_proba(self, X: pd.DataFrame) -> np.ndarray`**: Predicts class probabilities on new data.\n",
        "    * **`save(self, filepath: str) -> None`**: Saves the trained model (pipeline) to a file using `joblib`. It also creates the directory if it doesn't exist.\n",
        "    * **`log_run(self, directory: str, metrics: Dict[str, Any], dataset_info: Dict[str, Any], log_filename: str = \"churn_model_run_log.json\") -> None`**: Saves model configuration, performance metrics, and dataset information to a JSON log file. It appends to an existing log or creates a new one.\n",
        "\n",
        "* **Helper Functions**:\n",
        "    * **`compute_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, target_names: Optional[list[str]] = None) -> dict[str, Any]`**: Computes standard classification metrics (accuracy, precision, recall, f1-score, confusion matrix, classification report) for binary classification.\n",
        "    * **`report_classification_metrics(metrics: dict[str, Any]) -> None`**: Prints a formatted report of the classification metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14823ba1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "model.py\n",
        "\n",
        "This script defines the ChurnPredictionModel class, which encapsulates\n",
        "the machine learning model (e.g., Logistic Regression) and its associated\n",
        "preprocessing pipeline. It also includes functions for computing and reporting\n",
        "classification metrics, and for logging model run details.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Any, cast, Optional\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler # Import for fallback in __init__\n",
        "\n",
        "\n",
        "class ChurnPredictionModel:\n",
        "    \"\"\"\n",
        "    Customer churn prediction model using configurable classifier with preprocessing pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifier=None,\n",
        "        preprocessor=None,\n",
        "        random_state: int = 42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the churn prediction pipeline.\n",
        "\n",
        "        Args:\n",
        "            classifier: Scikit-learn classifier instance. If None, uses LogisticRegression.\n",
        "            preprocessor: Scikit-learn preprocessing pipeline. If None, uses StandardScaler only.\n",
        "            random_state (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.classifier = classifier if classifier is not None else LogisticRegression(\n",
        "            random_state=self.random_state,\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        self.pipe = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', self.classifier)\n",
        "        ]) if preprocessor is not None else Pipeline([\n",
        "            ('scaler', StandardScaler()), # Fallback if no preprocessor provided\n",
        "            ('classifier', self.classifier)\n",
        "        ])\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'ChurnPredictionModel':\n",
        "        \"\"\"\n",
        "        Fit the model to training data.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Training features.\n",
        "            y (pd.Series): Training target.\n",
        "\n",
        "        Returns:\n",
        "            ChurnPredictionModel: Self for method chaining.\n",
        "        \"\"\"\n",
        "        print(f\"Training model with {len(X)} samples and {len(X.columns)} features...\")\n",
        "\n",
        "        # Fit the pipeline\n",
        "        self.pipe.fit(X, y)\n",
        "\n",
        "        print(f\"Model trained successfully!\")\n",
        "        print(f\"Features used: {list(X.columns)}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions on input data.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "        return cast(np.ndarray, self.pipe.predict(X))\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict class probabilities.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class probabilities.\n",
        "        \"\"\"\n",
        "        return cast(np.ndarray, self.pipe.predict_proba(X))\n",
        "\n",
        "    def save(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained model to a file.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Full path to save the model.\n",
        "        \"\"\"\n",
        "        directory = os.path.dirname(filepath)\n",
        "        if directory and not os.path.exists(directory):\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        joblib.dump(self.pipe, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def log_run(\n",
        "        self,\n",
        "        directory: str,\n",
        "        metrics: dict[str, Any],\n",
        "        dataset_info: dict[str, Any],\n",
        "        log_filename: str = \"churn_model_run_log.json\"\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Save model configuration and performance metrics to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            directory (str): Directory where the JSON file will be stored.\n",
        "            metrics (Dict[str, Any]): Evaluation metrics to save.\n",
        "            dataset_info (Dict[str, Any]): Information about the dataset used.\n",
        "            log_filename (str): Name of the log file.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        run_info = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"model_class\": self.__class__.__name__,\n",
        "            \"classifier\": str(type(self.classifier).__name__),\n",
        "            \"dataset\": \"Customer Churn\",\n",
        "            \"dataset_info\": dataset_info,\n",
        "            \"parameters\": {\n",
        "                \"random_state\": self.random_state,\n",
        "                \"classifier_params\": self.classifier.get_params()\n",
        "            },\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "\n",
        "        log_file = os.path.join(directory, log_filename)\n",
        "\n",
        "        # Load existing logs or create new list\n",
        "        if os.path.exists(log_file):\n",
        "            try:\n",
        "                with open(log_file, \"r\") as f:\n",
        "                    logs = json.load(f)\n",
        "            except (json.JSONDecodeError, FileNotFoundError):\n",
        "                logs = []\n",
        "        else:\n",
        "            logs = []\n",
        "\n",
        "        logs.append(run_info)\n",
        "\n",
        "        # Save updated logs\n",
        "        with open(log_file, \"w\") as f:\n",
        "            json.dump(logs, f, indent=4, default=str)\n",
        "\n",
        "        print(f\"Run log saved to {log_file}\")\n",
        "\n",
        "\n",
        "def compute_classification_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    target_names: Optional[list[str]] = None\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes classification metrics for binary classification.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): True labels.\n",
        "        y_pred (np.ndarray): Predicted labels.\n",
        "        target_names (Optional[list[str]]): Names of target classes.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary containing all computed metrics.\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, average='binary'),\n",
        "        \"recall\": recall_score(y_true, y_pred, average='binary'),\n",
        "        \"f1_score\": f1_score(y_true, y_pred, average='binary'),\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
        "        \"classification_report\": classification_report(\n",
        "            y_true, y_pred,\n",
        "            target_names=target_names,\n",
        "            output_dict=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def report_classification_metrics(metrics: dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Prints formatted classification metrics.\n",
        "\n",
        "    Args:\n",
        "        metrics (Dict[str, Any]): Model evaluation metrics.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    output_lines.append(\"\\n\" + \"=\"*40)\n",
        "    output_lines.append(\"CLASSIFICATION METRICS\")\n",
        "    output_lines.append(\"=\"*40)\n",
        "\n",
        "    # Check for key existence to prevent KeyErrors if metrics dict is incomplete\n",
        "    output_lines.append(f\"Accuracy : {metrics.get('accuracy', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"Precision: {metrics.get('precision', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"Recall   : {metrics.get('recall', float('nan')):.4f}\")\n",
        "    output_lines.append(f\"F1-Score : {metrics.get('f1_score', float('nan')):.4f}\")\n",
        "\n",
        "    output_lines.append(f\"\\nConfusion Matrix:\")\n",
        "    confusion_matrix_data = metrics.get('confusion_matrix')\n",
        "    if isinstance(confusion_matrix_data, list):\n",
        "        for row in confusion_matrix_data:\n",
        "            output_lines.append(f\"  {row}\")\n",
        "    elif confusion_matrix_data is not None:\n",
        "        output_lines.append(f\"  Unexpected format for confusion matrix: {confusion_matrix_data}\")\n",
        "    else:\n",
        "        output_lines.append(f\"  Confusion matrix data not available.\")\n",
        "\n",
        "    print(\"\\n\".join(output_lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c675c07e",
      "metadata": {},
      "source": [
        "### pipeline.py\n",
        "\n",
        "Orchestrating pipeline stages in sequence.\n",
        "\n",
        "This script defines and orchestrates the end to end machine learning pipeline for customer churn prediction. It encapsulates the sequential steps from data loading and preprocessing to model training and evaluation.\n",
        "\n",
        "* **`run_churn_pipeline(...)` Function**: This is the main function that runs the entire ML pipeline.\n",
        "    * **Data Loading**: Uses `load_churn_dataset` to load raw data.\n",
        "    * **Data Preprocessing**: Calls `clean_churn_data` for cleaning, `transform_features` for feature engineering and scaling, `split_features_and_target` to separate features and target, and `stratified_split` for splitting data into training and testing sets while preserving target distribution.\n",
        "    * **Model Training**: Initializes and trains a `ChurnPredictionModel` instance.\n",
        "    * **Prediction and Evaluation**: Makes predictions on the test set and computes and reports classification metrics using `compute_classification_metrics` and `report_classification_metrics`.\n",
        "    * **Model and Log Saving**: Saves the trained model using `model.save` and logs the run details and metrics using `model.log_run`.\n",
        "    * **Output**: Returns the trained model instance and the evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9b1493",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "pipeline.py\n",
        "\n",
        "This script defines and orchestrates the end-to-end machine learning pipeline\n",
        "for customer churn prediction. It encapsulates the sequential steps from\n",
        "data loading and preprocessing to model training and evaluation.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "from src.data_loader import load_churn_dataset\n",
        "from src.preprocessing import clean_churn_data, transform_features, split_features_and_target, stratified_split\n",
        "from src.model import ChurnPredictionModel, compute_classification_metrics, report_classification_metrics\n",
        "\n",
        "\n",
        "def run_churn_pipeline(\n",
        "    data_file_path: str,\n",
        "    target_column: str,\n",
        "    numeric_columns: list[str],\n",
        "    categorical_columns: list[str],\n",
        "    test_size: float,\n",
        "    random_state: int,\n",
        "    model_dir_path: str,\n",
        "    model_filename: str,\n",
        "    log_filename: str\n",
        ") -> Tuple[ChurnPredictionModel, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs the complete customer churn prediction pipeline.\n",
        "\n",
        "    Args:\n",
        "        data_file_path (str): Path to the raw dataset CSV file.\n",
        "        target_column (str): Name of the target column.\n",
        "        numeric_columns (list[str]): List of numeric feature column names.\n",
        "        categorical_columns (list[str]): List of categorical feature column names.\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        random_state (int): Random seed for reproducibility.\n",
        "        model_dir_path (str): Directory where the trained model and logs will be saved.\n",
        "        model_filename (str): Name of the file to save the trained model.\n",
        "        log_filename (str): Name of the file to save the run logs.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[ChurnPredictionModel, Dict[str, Any]]:\n",
        "            - The trained ChurnPredictionModel instance.\n",
        "            - A dictionary containing the evaluation metrics.\n",
        "    \"\"\"\n",
        "    print(\"\\n1. Loading dataset...\")\n",
        "    df_raw = load_churn_dataset(data_file_path)\n",
        "\n",
        "    print(\"\\n2. Cleaning data...\")\n",
        "    df_clean = clean_churn_data(df_raw, target_column, numeric_columns, categorical_columns)\n",
        "\n",
        "    print(\"\\n3. Transforming features...\")\n",
        "    # If transform_features now returns only 2 items, adjust unpacking:\n",
        "    df_transformed, preprocessor = transform_features(\n",
        "        df_clean, target_column, numeric_columns, categorical_columns\n",
        "    )\n",
        "\n",
        "    # Extract dataset information\n",
        "    dataset_info = {\n",
        "        \"total_samples\": len(df_transformed),\n",
        "        \"n_features\": len(df_transformed.attrs.get('feature_columns', [])),\n",
        "        \"target_mapping\": df_transformed.attrs.get('target_mapping', {}),\n",
        "        \"target_names\": df_transformed.attrs.get('target_names', []),\n",
        "        \"churn_distribution\": df_transformed['target_encoded'].value_counts().to_dict(),\n",
        "        \"feature_columns\": df_transformed.attrs.get('feature_columns', []),\n",
        "        \"numeric_features\": df_transformed.attrs.get('numeric_features', []),\n",
        "        \"categorical_features\": df_transformed.attrs.get('categorical_features', [])\n",
        "    }\n",
        "\n",
        "    print(\"\\n4. Preparing features and target...\")\n",
        "    X, y = split_features_and_target(df_transformed)\n",
        "\n",
        "    print(\"\\n5. Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = stratified_split(X, y, test_size=test_size, seed=random_state)\n",
        "    print(f\"Training set: {len(X_train)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "    print(f\"Train churn rate: {y_train.mean():.3f}\")\n",
        "    print(f\"Test churn rate: {y_test.mean():.3f}\")\n",
        "\n",
        "    print(\"\\n6. Training model...\")\n",
        "    model = ChurnPredictionModel(preprocessor=preprocessor, random_state=random_state)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\n7. Making predictions...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"\\n8. Evaluating model...\")\n",
        "    metrics = compute_classification_metrics(\n",
        "        y_test.to_numpy(),\n",
        "        y_pred,\n",
        "        target_names=dataset_info['target_names']\n",
        "    )\n",
        "    report_classification_metrics(metrics)\n",
        "\n",
        "    print(\"\\n9. Saving model and logs...\")\n",
        "    model_file_path = os.path.join(model_dir_path, model_filename)\n",
        "    model.save(model_file_path)\n",
        "    model.log_run(model_dir_path, metrics, dataset_info, log_filename=log_filename)\n",
        "\n",
        "    print(f\"Model saved to: {model_file_path}\")\n",
        "    print(f\"Run log saved to: {os.path.join(model_dir_path, log_filename)}\")\n",
        "\n",
        "    return model, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ca564c",
      "metadata": {},
      "source": [
        "### main.py\n",
        "\n",
        "Entry point for triggering the pipeline.\n",
        "\n",
        "This is the main entry point for the Customer Churn Prediction ML pipeline. It orchestrates the entire process by calling the main pipeline function and handling overall execution flow.\n",
        "\n",
        "* **Purpose**: Serves as the primary script to initiate and manage the execution of the customer churn prediction machine learning pipeline.\n",
        "* **`main(output_base_dir: Optional[Path] = None) -> None` Function**:\n",
        "    * **Initialization**: Prints a starting message and constructs file paths for data and model artifacts, defaulting to the script's directory or using a provided `output_base_dir`.\n",
        "    * **Pipeline Execution**: Calls `run_churn_pipeline` (imported from `src.pipeline`) with all necessary configuration parameters defined in `src.config`.\n",
        "    * **Logging and Error Handling**: Prints a success message and the final model accuracy upon successful completion. Includes robust error handling to catch and report exceptions during pipeline execution.\n",
        "* **Entry Point (`if __name__ == \"__main__\":`)**: Ensures that the `main()` function is called when the script is executed directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c0425b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "main.py\n",
        "\n",
        "This is the main entry point for the Customer Churn Prediction ML pipeline.\n",
        "It orchestrates the entire process by calling the main pipeline function\n",
        "and handling overall execution flow.\n",
        "\"\"\"\n",
        "from typing import Optional\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "from src.pipeline import run_churn_pipeline # Import the new pipeline function\n",
        "from src.config import (\n",
        "    TARGET_COLUMN, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS, TEST_SIZE,\n",
        "    RANDOM_STATE, MODEL_FILENAME, LOG_FILENAME, DATA_DIR_NAME,\n",
        "    RAW_DATA_DIR_NAME, DATASET_FILENAME, MODEL_STORE_DIR)\n",
        "\n",
        "\n",
        "def main(output_base_dir: Optional[Path] = None) -> None:\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the entire ML pipeline.\n",
        "\n",
        "    Args:\n",
        "        output_base_dir (Path, optional): The base directory where\n",
        "                                         data and model artifacts should be\n",
        "                                         read from/saved to. If None,\n",
        "                                         defaults to the script's directory.\n",
        "    \"\"\"\n",
        "    print(\"Starting Customer Churn Prediction Pipeline...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Construct necessary paths\n",
        "        if output_base_dir is None:\n",
        "            # Default to script's directory for normal runs\n",
        "            base_path = Path(os.path.dirname(os.path.abspath(__file__)))\n",
        "        else:\n",
        "            # Use the provided base directory for testing\n",
        "            base_path = output_base_dir\n",
        "\n",
        "        data_file_path = base_path / DATA_DIR_NAME / RAW_DATA_DIR_NAME / DATASET_FILENAME\n",
        "        model_dir_path = base_path / MODEL_STORE_DIR\n",
        "\n",
        "        # Run the entire churn prediction pipeline\n",
        "        trained_model, evaluation_metrics = run_churn_pipeline(\n",
        "            data_file_path=str(data_file_path),\n",
        "            target_column=TARGET_COLUMN,\n",
        "            numeric_columns=NUMERIC_COLUMNS,\n",
        "            categorical_columns=CATEGORICAL_COLUMNS,\n",
        "            test_size=TEST_SIZE,\n",
        "            random_state=RANDOM_STATE,\n",
        "            model_dir_path=str(model_dir_path),\n",
        "            model_filename=MODEL_FILENAME,\n",
        "            log_filename=LOG_FILENAME\n",
        "        )\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        print(f\"Final Model Accuracy: {evaluation_metrics['accuracy']:.4f}\") # Using metrics from pipeline output\n",
        "\n",
        "    except Exception as e:\n",
        "        #print(f\"\\nERROR: Pipeline failed with exception: {e}\")\n",
        "        print(f\"ERROR: Pipeline failed with exception: {e}\", file=sys.stderr) # Direct output to stderr\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
