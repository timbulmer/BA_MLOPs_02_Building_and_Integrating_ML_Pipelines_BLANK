{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6182c51c",
      "metadata": {},
      "source": [
        "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlflow-explanation",
      "metadata": {},
      "source": [
        "# MLflow\n",
        "\n",
        "## Simple Model Deployment and Evaluation\n",
        "\n",
        "Let's walk through a basic workflow demonstrating how MLflow can be used to track experiments, log parameters, metrics, and models for a simple classification task.\n",
        "\n",
        "### What is MLflow?\n",
        "\n",
        "MLflow is an open-source platform for managing the end to end machine learning lifecycle. It addresses four primary functions:\n",
        "\n",
        "* **Tracking:** Record and query experiments (code, data, config, results).\n",
        "\n",
        "* **Projects:** Package ML code in a reusable, reproducible form.\n",
        "\n",
        "* **Models:** Manage and deploy models from various ML libraries to diverse serving platforms.\n",
        "\n",
        "* **Model Registry:** Centralized model store to collaboratively manage the full lifecycle of MLflow Models.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "This example demonstrates a basic MLflow workflow for a binary classification problem using a Decision Tree Classifier. It covers:\n",
        "\n",
        "1. **Setting up an MLflow Experiment:** Organizing runs under a common name.\n",
        "\n",
        "2. **Starting an MLflow Run:** Creating a specific instance of an experiment.\n",
        "\n",
        "3. **Data Preparation:** Generating synthetic data for a simplified binary classification.\n",
        "\n",
        "4. **Logging Parameters:** Recording key data split parameters and model hyperparameters.\n",
        "\n",
        "5. **Model Training:** Training a `DecisionTreeClassifier` on the prepared data.\n",
        "\n",
        "6. **Calculating and Logging Metrics:** Evaluating the model's performance (accuracy, precision, recall, F1-score) and logging these metrics.\n",
        "\n",
        "7. **Logging the Model:** Saving the trained model as an MLflow artifact, making it ready for deployment.\n",
        "\n",
        "This setup allows for easy comparison of different runs, understanding the impact of various parameters, and reproducing results.\n",
        "\n",
        "### Installation & Setup\n",
        "\n",
        "First, you'd install MLflow in your Python environment:\n",
        "\n",
        "* `pip install mlflow`\n",
        "\n",
        "Lets review a simple classification example where we create some data and applies a decision tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "019c8d67",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 12:07:36 INFO mlflow.tracking.fluent: Experiment with name 'Decision_Tree_Classification_Analysis_Simplified' does not exist. Creating a new experiment.\n",
            "2025/07/04 12:07:37 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/04 12:07:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow Run completed for Experiment '707572921101383408' and Run '60f5b250b43f432ea9b01fd51fb1762b'\n",
            "Logged Data Split: Test Ratio=0.2, Stratified=True\n",
            "Logged Model Parameters: Max Depth=4, Min Samples Leaf=3, Criterion=entropy\n",
            "Logged Metrics: Accuracy=0.8000, Precision=0.7500, Recall=0.7500, F1-Score=0.7500\n",
            "Model saved as artifact: decision_tree_classifier_model\n",
            "To view this run, run 'mlflow ui' in your terminal in this directory.\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Set the MLflow Experiment ---\n",
        "mlflow.set_experiment(\"Decision_Tree_Classification_Analysis_Simplified\")\n",
        "\n",
        "# --- 2. Start an MLflow Run ---\n",
        "with mlflow.start_run(run_name=\"Decision_Tree_Trial_Simplified_Data\"):\n",
        "    \n",
        "    # --- Data Preparation for Binary Classification ---\n",
        "    np.random.seed(42) # For reproducibility of random data\n",
        "\n",
        "    # Create two simple features\n",
        "    X = pd.DataFrame({\n",
        "        'feature_1': np.random.rand(100) * 10,  # Random values between 0 and 10\n",
        "        'feature_2': np.random.rand(100) * 10\n",
        "    })\n",
        "\n",
        "    # Create a simplified binary target:\n",
        "    # If feature_1 is high (e.g., > 5), assign class 1. Otherwise, assign class 0.\n",
        "    # This creates a clear, easy-to-understand separation for the target variable.\n",
        "    y = (X['feature_1'] > 5).astype(int) \n",
        "    # Add a little noise to 'y' to make it less perfectly separable, simulating real-world data\n",
        "    # We'll flip a small percentage of labels\n",
        "    noise_idx = np.random.choice(len(y), size=10, replace=False) # Select 10 random indices\n",
        "    y.iloc[noise_idx] = 1 - y.iloc[noise_idx] # Flip the labels at these indices\n",
        "\n",
        "    # --- Log Data Split Parameters ---\n",
        "    # These are crucial for understanding how the data was split for training/testing.\n",
        "    test_split_ratio = 0.20 # 20% of data for testing\n",
        "    is_stratified_split = True # Indicates if the split maintained class proportions\n",
        "\n",
        "    mlflow.log_param(\"data_test_split_ratio\", test_split_ratio)\n",
        "    mlflow.log_param(\"data_is_stratified_split\", is_stratified_split)\n",
        "    \n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, \n",
        "        test_size=test_split_ratio, \n",
        "        random_state=42, \n",
        "        stratify=y if is_stratified_split else None # Apply stratification based on our new parameter\n",
        "    )\n",
        "\n",
        "    # --- Log Model Parameters ---\n",
        "    max_depth_param = 4       # Max depth of the tree\n",
        "    min_samples_leaf_param = 3 # Minimum samples required at a leaf node\n",
        "    criterion_param = \"entropy\"  # Information gain for splitting\n",
        "    \n",
        "    mlflow.log_param(\"model_max_depth\", max_depth_param)\n",
        "    mlflow.log_param(\"model_min_samples_leaf\", min_samples_leaf_param)\n",
        "    mlflow.log_param(\"model_criterion\", criterion_param)\n",
        "    mlflow.log_param(\"input_features_count\", X.shape[1])\n",
        "\n",
        "    # --- Train a Decision Tree model ---\n",
        "    model = DecisionTreeClassifier(max_depth=max_depth_param, \n",
        "                                   min_samples_leaf=min_samples_leaf_param,\n",
        "                                   criterion=criterion_param,\n",
        "                                   random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # --- Make predictions and calculate metrics ---\n",
        "    predictions = model.predict(X_test)\n",
        "    \n",
        "    # --- Log Metrics ---\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, zero_division=0)\n",
        "    recall = recall_score(y_test, predictions, zero_division=0)\n",
        "    f1 = f1_score(y_test, predictions, zero_division=0)\n",
        "\n",
        "    mlflow.log_metric(\"evaluation_accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"evaluation_precision\", precision)\n",
        "    mlflow.log_metric(\"evaluation_recall\", recall)\n",
        "    mlflow.log_metric(\"evaluation_f1_score\", f1)\n",
        "\n",
        "    # --- 5. Log the Model (as an Artifact) ---\n",
        "    mlflow.sklearn.log_model(model, \"decision_tree_classifier_model\")\n",
        "\n",
        "    print(f\"MLflow Run completed for Experiment '{mlflow.active_run().info.experiment_id}' and Run '{mlflow.active_run().info.run_id}'\")\n",
        "    print(f\"Logged Data Split: Test Ratio={test_split_ratio}, Stratified={is_stratified_split}\")\n",
        "    print(f\"Logged Model Parameters: Max Depth={max_depth_param}, Min Samples Leaf={min_samples_leaf_param}, Criterion={criterion_param}\")\n",
        "    print(f\"Logged Metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1:.4f}\")\n",
        "    print(f\"Model saved as artifact: decision_tree_classifier_model\")\n",
        "    print(f\"To view this run, run 'mlflow ui' in your terminal in this directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36a1905",
      "metadata": {},
      "source": [
        "**How to see your results: The MLflow UI**\n",
        "- After running this script, open your terminal in the directory where your 'mlruns' folder is located.\n",
        "- Then, execute:\n",
        "    - `mlflow ui`\n",
        "    - This will launch the MLflow Tracking UI, usually accessible at http://localhost:5000.\n",
        "    - You'll be able to navigate to the \"Decision_Tree_Classification_Analysis\" experiment and inspect the details of \"Decision_Tree_Trial_1\", including its parameters, metrics, and the saved model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf74cd0a",
      "metadata": {},
      "source": [
        "### MLflow Tracking Cheat Sheet: Essential Logging Functions\n",
        "\n",
        "Here a table that summarises the most common and essential `mlflow.log_*` functions, along with their purpose and examples. This will serve as a quick reference for effectively leveraging MLflow Tracking.\n",
        "\n",
        "| MLflow Function | Purpose | When to Use | Example |\n",
        "| :--------------------------- | :------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------- |\n",
        "| **`mlflow.log_param(key, value)`** | **Logs a single key-value parameter.** Values are typically strings, numbers, or booleans. Used for hyperparameters, configuration settings, or data characteristics. | To record **input settings** or **configurations** that define a specific run. Essential for reproducibility and comparison. | `mlflow.log_param(\"learning_rate\", 0.01)` <br> `mlflow.log_param(\"optimizer\", \"Adam\")` <br> `mlflow.log_param(\"data_scaling_method\", \"MinMaxScaler\")` |\n",
        "| **`mlflow.log_metric(key, value)`** | **Logs a single key-value metric.** Values must be numeric. MLflow tracks metrics as they change over time (e.g., during training epochs). | To record **quantitative performance indicators** of your model or experiment. Essential for comparing run performance. | `mlflow.log_metric(\"accuracy\", 0.925)` <br> `mlflow.log_metric(\"validation_loss\", 0.15, step=10)` (for epoch-based logging) <br> `mlflow.log_metric(\"f1_score\", 0.88)` |\n",
        "| **`mlflow.log_artifact(local_path, artifact_path=None)`** | **Logs a single file as an artifact.** The file is copied from `local_path` to the run's artifact URI. | To save **important output files** that are not parameters or metrics but provide context or are useful for later analysis/reproducibility. | `mlflow.log_artifact(\"plots/roc_curve.png\")` <br> `mlflow.log_artifact(\"my_dataset_sample.csv\", artifact_path=\"data\")` |\n",
        "| **`mlflow.log_artifacts(local_dir, artifact_path=None)`** | **Logs an entire directory of files as artifacts.** All files within `local_dir` are recursively copied. | To save **multiple related output files** at once (e.g., all plots from a run, all config files). | `mlflow.log_artifacts(\"results_folder\")` <br> `mlflow.log_artifacts(\"configs/\", artifact_path=\"model_configs\")` |\n",
        "| **`mlflow.<flavor>.log_model(model, artifact_path, ...)`** | **Logs a machine learning model specific to a framework/library (flavor).** This function packages the model along with necessary metadata, code dependencies, and often a signature. | To save your **trained ML model** in a standardized format that MLflow can later reload, serve, and deploy using various tools. **This is crucial for model management!** | `mlflow.sklearn.log_model(my_sklearn_model, \"my_classification_model\")` <br> `mlflow.pytorch.log_model(my_torch_model, \"pytorch_model\")` <br> `mlflow.tensorflow.log_model(my_tf_model, \"tf_model\")` |\n",
        "| **`mlflow.set_experiment(experiment_name)`** | **Sets the current active experiment.** If the experiment doesn't exist, it's created. All subsequent runs will belong to this experiment. | To **organize your runs** into logical groups. This is typically one of the first lines of MLflow code in a script. | `mlflow.set_experiment(\"Customer_Churn_Prediction\")` <br> `mlflow.set_experiment(\"Image_Recognition_CNN_Trials\")` |\n",
        "| **`mlflow.set_tag(key, value)`** | **Adds a tag (key-value pair) to the current run.** Tags are flexible strings used for arbitrary metadata. | To add **descriptive or categorizing metadata** that doesn't fit into parameters or metrics (e.g., developer name, data version, specific algorithm variant). | `mlflow.set_tag(\"developer\", \"Alice\")` <br> `mlflow.set_tag(\"data_version\", \"v2.1_cleaned\")` <br> `mlflow.set_tag(\"model_family\", \"tree_based\")` |\n",
        "| **`mlflow.start_run(run_name=None, ...)`** | **Starts an MLflow run and returns an `ActiveRun` object.** Typically used with a `with` statement for automatic run termination. | To **begin tracking a new experiment run.** This is the core context manager for capturing run information. | `with mlflow.start_run(run_name=\"Hyperparam_Search_Trial_1\"):` <br> `  # Your ML code here` |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "multiple-models-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Multiple Models Comparison\n",
        "\n",
        "## Comparing Different Classification Models on the Iris Dataset\n",
        "\n",
        "This section demonstrates how to use MLflow to track and compare the performance of multiple machine learning models within a single experiment. We will use the classic **Iris dataset** for a multi-class classification problem.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary goal of this example is to illustrate MLflow's capability to facilitate model comparison by logging distinct runs for different models under a unified experiment. Specifically, it covers:\n",
        "\n",
        "1.  **Experiment Setup:** Defining a new MLflow experiment (`Iris Classification Models`) to group all runs related to Iris classification.\n",
        "2.  **Dataset Loading and Splitting:** Loading the Iris dataset and splitting it into training and testing sets.\n",
        "3.  **Iterative Model Training and Logging:** Training three different classification models:\n",
        "    * **K-Nearest Neighbors (KNN)**\n",
        "    * **Support Vector Machine (SVM)**\n",
        "    * **Decision Tree Classifier**\n",
        "4.  **Parameter Tracking:** For each model, relevant hyperparameters (e.g., `n_neighbors` for KNN, `C` and `kernel` for SVM, `max_depth` for Decision Tree) are logged using `mlflow.log_param()`.\n",
        "5.  **Metric Logging:** Standard classification metrics (Accuracy, Precision, Recall, F1-Score) are calculated on the test set and logged for each model using `mlflow.log_metric()`. The `average='weighted'` parameter is used for precision, recall, and F1 score to handle the multi class nature of the Iris dataset.\n",
        "6.  **Model Artifacts:** Each trained model is saved as an MLflow artifact using `mlflow.sklearn.log_model()`, allowing for easy retrieval and deployment later.\n",
        "\n",
        "By logging each model's parameters and performance metrics within separate runs under the same experiment, MLflow provides a clear interface to compare their effectiveness and identify the best performing model for the given task.\n",
        "\n",
        "### How it Works\n",
        "\n",
        "The code uses a `with mlflow.start_run(...)` block for each model. This ensures that all parameters, metrics, and the model logged within that block are associated with that specific run. The experiment name is set once at the beginning, grouping all these individual model runs together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6430670",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Iris Classification Model Comparison...\n",
            "\n",
            "--- Training KNN Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:01:22 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/04 14:01:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025/07/04 14:01:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  KNN Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  KNN Model saved to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/227057849671184419/689e3191300a469d881cde3077e7db59/artifacts/knn_model\n",
            "\n",
            "--- Training SVM Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:01:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "2025/07/04 14:01:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SVM Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  SVM Model saved to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/227057849671184419/1c13cd53b6754b70ba8c6f859360c591/artifacts/svm_model\n",
            "\n",
            "--- Training Decision Tree Classifier ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:01:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Decision Tree Metrics: Accuracy=1.0000, Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "  Decision Tree Model saved to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/227057849671184419/2cfad32c56d3403c8c44eeba2ebdf492/artifacts/decision_tree_model\n",
            "\n",
            "All Iris classification models trained and logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Iris Classification Models' experiment.\n"
          ]
        }
      ],
      "source": [
        "##Difernt models isis:\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names # To map target indices to names\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Set MLflow Experiment ---\n",
        "# It's crucial to set the experiment name *before* starting any runs if you want them grouped.\n",
        "mlflow.set_experiment(\"Iris Classification Models\")\n",
        "\n",
        "print(\"Starting Iris Classification Model Comparison...\")\n",
        "\n",
        "# --- Run 1: K-Nearest Neighbors (KNN) ---\n",
        "with mlflow.start_run(run_name=\"KNN Classifier\"):\n",
        "    print(\"\\n--- Training KNN Classifier ---\")\n",
        "    # Define hyperparameters\n",
        "    n_neighbors = 5\n",
        "    weights = \"uniform\"\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"K-Nearest Neighbors\")\n",
        "    mlflow.log_param(\"n_neighbors\", n_neighbors)\n",
        "    mlflow.log_param(\"weights\", weights)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
        "    knn_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = knn_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # For multiclass, precision, recall, f1_score need 'average' parameter\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"precision\", precision)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(knn_model, \"knn_model\")\n",
        "\n",
        "    print(f\"  KNN Metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1:.4f}\")\n",
        "    print(f\"  KNN Model saved to: {mlflow.active_run().info.artifact_uri}/knn_model\")\n",
        "\n",
        "\n",
        "# --- Run 2: Support Vector Machine (SVM) ---\n",
        "with mlflow.start_run(run_name=\"SVM Classifier\"):\n",
        "    print(\"\\n--- Training SVM Classifier ---\")\n",
        "    # Define hyperparameters\n",
        "    C = 1.0\n",
        "    kernel = \"rbf\"\n",
        "    gamma = \"scale\"\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"Support Vector Machine\")\n",
        "    mlflow.log_param(\"C\", C)\n",
        "    mlflow.log_param(\"kernel\", kernel)\n",
        "    mlflow.log_param(\"gamma\", gamma)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    svm_model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)\n",
        "    svm_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"precision\", precision)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(svm_model, \"svm_model\")\n",
        "\n",
        "    print(f\"  SVM Metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1:.4f}\")\n",
        "    print(f\"  SVM Model saved to: {mlflow.active_run().info.artifact_uri}/svm_model\")\n",
        "\n",
        "\n",
        "# --- Run 3: Decision Tree Classifier ---\n",
        "with mlflow.start_run(run_name=\"Decision Tree Classifier\"):\n",
        "    print(\"\\n--- Training Decision Tree Classifier ---\")\n",
        "    # Define hyperparameters\n",
        "    max_depth = 5\n",
        "    min_samples_leaf = 5\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"Decision Tree\")\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "    mlflow.log_param(\"min_samples_leaf\", min_samples_leaf)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    dt_model = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=42)\n",
        "    dt_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = dt_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"precision\", precision)\n",
        "    mlflow.log_metric(\"recall\", recall)\n",
        "    mlflow.log_metric(\"f1_score\", f1)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(dt_model, \"decision_tree_model\")\n",
        "\n",
        "    print(f\"  Decision Tree Metrics: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1-Score={f1:.4f}\")\n",
        "    print(f\"  Decision Tree Model saved to: {mlflow.active_run().info.artifact_uri}/decision_tree_model\")\n",
        "\n",
        "print(\"\\nAll Iris classification models trained and logged to MLflow.\")\n",
        "print(\"To view results, run 'mlflow ui' in your terminal and navigate to the 'Iris Classification Models' experiment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d4b0d5",
      "metadata": {},
      "source": [
        "In essence, this code runs three separate machine learning classification experiments on the same dataset (Iris) and uses MLflow to automatically record all the key details—the model's settings (parameters), its performance (metrics), and the trained model itself (artifacts)—for each experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "titanic-mlflow-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Preprocessing, Multiple Models, and Cross-Validation\n",
        "\n",
        "## Robust Machine Learning Workflow for Titanic Survival Prediction\n",
        "\n",
        "This example demonstrates a comprehensive machine learning workflow for the Kaggle Titanic survival prediction problem. It focuses on building robust, evaluable, and traceable models by incorporating data preprocessing, comparing multiple model types, and utilizing cross-validation, all tracked meticulously with MLflow.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The main objective is to showcase an end-to-end ML pipeline within the MLflow framework, covering:\n",
        "\n",
        "* **Data Loading and Initial Cleaning:** Handling the input CSV files and dropping irrelevant columns like `PassengerId`, `Name`, `Ticket`, and `Cabin`.\n",
        "\n",
        "* **Automated Preprocessing Pipelines:** Defining and applying robust preprocessing steps for both numerical and categorical features. This includes:\n",
        "    * **Numerical Features (`Age`, `Fare`, `SibSp`, `Parch`):** Imputing missing values with the mean and scaling features using `StandardScaler`.\n",
        "    * **Categorical Features (`Pclass`, `Sex`, `Embarked`):** Imputing missing values with the most frequent value and converting categorical data into numerical format using `OneHotEncoder`.\n",
        "    * These steps are encapsulated within `sklearn.pipeline.Pipeline` and combined using `sklearn.compose.ColumnTransformer` for a streamlined process.\n",
        "\n",
        "* **MLflow Experiment Setup:** Setting up a dedicated MLflow experiment (`Titanic CV & Model Logging`) to group all runs related to this project, ensuring organized tracking.\n",
        "\n",
        "* **Cross-Validation for Robust Evaluation:** Employing `StratifiedKFold` cross-validation to assess model performance more reliably across different subsets of the training data. This helps in getting a more generalized estimate of the model's performance and reducing overfitting bias.\n",
        "\n",
        "* **Multiple Model Comparison:** Training and evaluating three distinct classification algorithms:\n",
        "    * **Logistic Regression:** A linear model for binary classification.\n",
        "    * **Random Forest Classifier:** An ensemble tree-based method known for its robustness.\n",
        "    * **Gradient Boosting Classifier:** Another powerful ensemble method that builds trees sequentially.\n",
        "\n",
        "* **Comprehensive MLflow Logging:** For each model and cross-validation run, MLflow logs are captured:\n",
        "    * **Parameters:** Model hyperparameters, data split configurations, and preprocessing details.\n",
        "    * **Metrics:** Mean and standard deviation of cross-validation scores (accuracy, precision, recall, F1-score) to provide a statistical summary of performance.\n",
        "    * **Models as Artifacts:** The entire `sklearn.pipeline.Pipeline` (including preprocessing and the final classifier) is logged as an MLflow artifact, making it easy to reproduce predictions or deploy the exact trained workflow.\n",
        "\n",
        "This structured approach ensures that every aspect of the model development process is recorded, enabling easy comparison, reproducibility, and auditing of machine learning experiments.\n",
        "\n",
        "### Data Requirements\n",
        "\n",
        "This script expects `train.csv` and `test.csv` files from the Kaggle Titanic competition to be present in a `data/` subdirectory relative to where the script is run. If these files are not found, the script will exit and provide instructions for downloading them.\n",
        "\n",
        "### How to See Your Results: The MLflow UI\n",
        "\n",
        "After running this script, you can visualize and compare all the logged experiments and runs using the MLflow Tracking UI:\n",
        "\n",
        "-   Open your terminal in the directory where your 'mlruns' folder is located (this is usually the directory from which you ran the Python script).\n",
        "-   Execute the command:\n",
        "    -   `mlflow ui`\n",
        "-   This will launch the MLflow Tracking UI, typically accessible in your web browser at `http://localhost:5000`.\n",
        "-   You'll be able to navigate to the `Titanic CV & Model Logging` experiment and inspect the details of each model's run, including logged parameters, metrics (mean and std of CV scores), and the saved `final_model_pipeline` artifact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f586ebf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "N_SPLITS_CV = 5 # Number of folds for cross-validation\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "# You need train.csv and test.csv from Kaggle Titanic competition\n",
        "TRAIN_FILE = \"data/train.csv\"\n",
        "TEST_FILE = \"data/test.csv\"\n",
        "\n",
        "if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "    print(f\"Required files ({TRAIN_FILE}, {TEST_FILE}) not found.\")\n",
        "    print(\"Please download them from Kaggle Titanic competition:\")\n",
        "    print(\"https://www.kaggle.com/c/titanic/data\")\n",
        "    exit()\n",
        "\n",
        "df_train = pd.read_csv(TRAIN_FILE)\n",
        "df_test = pd.read_csv(TEST_FILE) # This is the \"unseen\" Kaggle test set\n",
        "\n",
        "# Store PassengerId for potential use, though not used for submission anymore\n",
        "test_passenger_ids = df_test['PassengerId']\n",
        "\n",
        "# Drop 'PassengerId', 'Name', 'Ticket', 'Cabin' from both train and test data for this example\n",
        "cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "df_train_processed = df_train.drop(columns=[col for col in cols_to_drop if col in df_train.columns])\n",
        "df_test_processed = df_test.drop(columns=[col for col in cols_to_drop if col in df_test.columns])\n",
        "\n",
        "\n",
        "# Define target and features\n",
        "target = 'Survived'\n",
        "X_full = df_train_processed.drop(target, axis=1)\n",
        "y_full = df_train_processed[target]\n",
        "\n",
        "# Define feature types for preprocessing\n",
        "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "# --- 2. Define Preprocessing Pipeline (with Cleaning Steps) ---\n",
        "\n",
        "# Preprocessing for numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# --- Set MLflow Experiment ---\n",
        "mlflow.set_experiment(\"Titanic CV & Model Logging\") # Renamed experiment slightly\n",
        "print(\"Starting Titanic Classification with CV and Model Logging...\")\n",
        "\n",
        "# Helper function to run CV, evaluate, and log to MLflow\n",
        "def train_evaluate_log_model(model_name, classifier, X_full, y_full, X_test_kaggle, test_passenger_ids, model_params):\n",
        "    with mlflow.start_run(run_name=f\"{model_name}_CV_Run\"): # Renamed run name slightly\n",
        "        print(f\"\\n--- Training {model_name} with Cross-Validation ---\")\n",
        "\n",
        "        # Log parameters specific to this run\n",
        "        mlflow.log_param(\"model_type\", model_name)\n",
        "        mlflow.log_param(\"n_splits_cv\", N_SPLITS_CV)\n",
        "        mlflow.log_param(\"cleaning_steps\", \"Age Impute Mean, Embarked Impute Mode\")\n",
        "        mlflow.log_param(\"preprocessing_steps\", \"StandardScaler, OneHotEncoder\")\n",
        "        mlflow.log_param(\"numerical_features\", str(numerical_features))\n",
        "        mlflow.log_param(\"categorical_features\", str(categorical_features))\n",
        "        mlflow.log_params(model_params)\n",
        "\n",
        "        # Create the full pipeline including preprocessing and classifier\n",
        "        full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                        ('classifier', classifier)])\n",
        "\n",
        "        # --- Perform Cross-Validation ---\n",
        "        cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=42)\n",
        "        scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
        "        cv_results = cross_validate(full_pipeline, X_full, y_full, cv=cv, scoring=scoring, return_train_score=False)\n",
        "\n",
        "        # Log mean and standard deviation of CV metrics\n",
        "        print(f\"  {model_name} CV Results:\")\n",
        "        for metric in scoring:\n",
        "            mean_metric = np.mean(cv_results[f'test_{metric}'])\n",
        "            std_metric = np.std(cv_results[f'test_{metric}'])\n",
        "            mlflow.log_metric(f\"cv_mean_{metric}\", mean_metric)\n",
        "            mlflow.log_metric(f\"cv_std_{metric}\", std_metric)\n",
        "            print(f\"    Mean Test {metric}: {mean_metric:.4f} (Std: {std_metric:.4f})\")\n",
        "\n",
        "        # --- Train the final model on the full training data ---\n",
        "        # This model will be saved to MLflow artifacts\n",
        "        print(f\"  Fitting final {model_name} pipeline on full training data for logging...\")\n",
        "        full_pipeline.fit(X_full, y_full)\n",
        "\n",
        "        # Log the final trained pipeline\n",
        "        mlflow.sklearn.log_model(full_pipeline, \"final_model_pipeline\")\n",
        "        print(f\"  Final {model_name} pipeline logged to: {mlflow.active_run().info.artifact_uri}/final_model_pipeline\")\n",
        "\n",
        "        # Note: Predictions on X_test_kaggle and submission file saving are removed.\n",
        "\n",
        "\n",
        "# --- Define Models and their Parameters ---\n",
        "models_to_compare = [\n",
        "    {\n",
        "        \"name\": \"Logistic Regression\",\n",
        "        \"classifier\": LogisticRegression(C=0.1, solver='liblinear', max_iter=1000, random_state=42),\n",
        "        \"params\": {\"C\": 0.1, \"solver\": \"liblinear\", \"max_iter\": 1000}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Random Forest Classifier\",\n",
        "        \"classifier\": RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42),\n",
        "        \"params\": {\"n_estimators\": 100, \"max_depth\": 8}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Gradient Boosting Classifier\",\n",
        "        \"classifier\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
        "        \"params\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3}\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Run the full pipeline for each model ---\n",
        "for model_info in models_to_compare:\n",
        "    train_evaluate_log_model(\n",
        "        model_info[\"name\"],\n",
        "        model_info[\"classifier\"],\n",
        "        X_full,\n",
        "        y_full,\n",
        "        df_test_processed, # X_test_kaggle is still passed, but not used for predictions in this version\n",
        "        test_passenger_ids, # test_passenger_ids is still passed, but not used in this version\n",
        "        model_info[\"params\"]\n",
        "    )\n",
        "\n",
        "print(\"\\nAll Titanic models processed with CV and models logged to MLflow.\")\n",
        "print(\"To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b05c87",
      "metadata": {},
      "source": [
        "In essence, this code sets up a robust machine learning workflow for the Titanic dataset that automatically handles data preparation and evaluates multiple models using cross-validation, while meticulously logging all parameters, cross-validation metrics, and the final trained models to MLflow for easy comparison and reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d35723",
      "metadata": {},
      "source": [
        "After some refractoring: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "28472335",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Titanic Classification with CV and Model Logging...\n",
            "\n",
            "--- Training Logistic Regression with Cross-Validation ---\n",
            "  Logistic Regression CV Results:\n",
            "    Mean Test accuracy: 0.8014 (Std: 0.0134)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Mean Test precision_weighted: 0.8002 (Std: 0.0136)\n",
            "    Mean Test recall_weighted: 0.8014 (Std: 0.0134)\n",
            "    Mean Test f1_weighted: 0.7988 (Std: 0.0135)\n",
            "  Fitting final Logistic Regression pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Logistic Regression pipeline logged to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/142701399195007350/680e1ef1e0124ac0839a5a92684bb404/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Random Forest Classifier with Cross-Validation ---\n",
            "  Random Forest Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8406 (Std: 0.0170)\n",
            "    Mean Test precision_weighted: 0.8418 (Std: 0.0179)\n",
            "    Mean Test recall_weighted: 0.8406 (Std: 0.0170)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Mean Test f1_weighted: 0.8374 (Std: 0.0177)\n",
            "  Fitting final Random Forest Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Random Forest Classifier pipeline logged to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/142701399195007350/aad05072b85e4de483d9742f928ae935/artifacts/final_model_pipeline\n",
            "\n",
            "--- Training Gradient Boosting Classifier with Cross-Validation ---\n",
            "  Gradient Boosting Classifier CV Results:\n",
            "    Mean Test accuracy: 0.8227 (Std: 0.0152)\n",
            "    Mean Test precision_weighted: 0.8235 (Std: 0.0145)\n",
            "    Mean Test recall_weighted: 0.8227 (Std: 0.0152)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Mean Test f1_weighted: 0.8191 (Std: 0.0167)\n",
            "  Fitting final Gradient Boosting Classifier pipeline on full training data for logging...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/07/04 14:25:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Final Gradient Boosting Classifier pipeline logged to: file:///C:/Users/MiguelAngelSanchezRa/Python%20Sandbox/Corporate%20training/BA%20MLOps/02%20Building%20%26%20Integrating%20ML%20Pipelines/ML_Flow/mlruns/142701399195007350/3e09e630631343638cb3f5a099e088d7/artifacts/final_model_pipeline\n",
            "\n",
            "All Titanic models processed with CV and models logged to MLflow.\n",
            "To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration (Constants defined globally or passed as arguments) ---\n",
        "N_SPLITS_CV = 5 # Number of folds for cross-validation\n",
        "TRAIN_FILE = \"data/train.csv\"\n",
        "TEST_FILE = \"data/test.csv\"\n",
        "\n",
        "# --- Helper function to run CV, evaluate, and log to MLflow ---\n",
        "# This function encapsulates the logic for a single model's training and logging\n",
        "# It now correctly accepts 'preprocessor' and 'N_SPLITS_CV' as arguments\n",
        "def train_evaluate_log_model(model_name, classifier, X_full, y_full, model_params, preprocessor, n_splits_cv_param):\n",
        "    \"\"\"\n",
        "    Trains, evaluates (with cross-validation), and logs a machine learning model to MLflow.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): A descriptive name for the model (e.g., \"Logistic Regression\").\n",
        "        classifier (sklearn estimator): The scikit-learn classifier object (e.g., LogisticRegression()).\n",
        "        X_full (pd.DataFrame): The full feature dataset for training.\n",
        "        y_full (pd.Series): The full target dataset for training.\n",
        "        model_params (dict): Dictionary of hyperparameters for the classifier.\n",
        "        preprocessor (sklearn.compose.ColumnTransformer): The defined data preprocessing pipeline.\n",
        "        n_splits_cv_param (int): Number of folds for cross-validation.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=f\"{model_name}_CV_Run\"):\n",
        "        print(f\"\\n--- Training {model_name} with Cross-Validation ---\")\n",
        "\n",
        "        # Log parameters specific to this run\n",
        "        mlflow.log_param(\"model_type\", model_name)\n",
        "        mlflow.log_param(\"n_splits_cv\", n_splits_cv_param)\n",
        "        mlflow.log_param(\"cleaning_steps\", \"Age Impute Mean, Embarked Impute Mode\")\n",
        "        mlflow.log_param(\"preprocessing_steps\", \"StandardScaler, OneHotEncoder\")\n",
        "        # Log lists as strings for simplicity in MLflow UI param table\n",
        "        mlflow.log_param(\"numerical_features\", str(['Age', 'Fare', 'SibSp', 'Parch']))\n",
        "        mlflow.log_param(\"categorical_features\", str(['Pclass', 'Sex', 'Embarked']))\n",
        "        mlflow.log_params(model_params) # Log the specific model hyperparameters\n",
        "\n",
        "        # Create the full pipeline including preprocessing and classifier\n",
        "        full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                         ('classifier', classifier)])\n",
        "\n",
        "        # --- Perform Cross-Validation ---\n",
        "        cv = StratifiedKFold(n_splits=n_splits_cv_param, shuffle=True, random_state=42)\n",
        "        scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
        "        cv_results = cross_validate(full_pipeline, X_full, y_full, cv=cv, scoring=scoring, return_train_score=False)\n",
        "\n",
        "        # Log mean and standard deviation of CV metrics\n",
        "        print(f\"  {model_name} CV Results:\")\n",
        "        for metric in scoring:\n",
        "            mean_metric = np.mean(cv_results[f'test_{metric}'])\n",
        "            std_metric = np.std(cv_results[f'test_{metric}'])\n",
        "            mlflow.log_metric(f\"cv_mean_{metric}\", mean_metric)\n",
        "            mlflow.log_metric(f\"cv_std_{metric}\", std_metric)\n",
        "            print(f\"    Mean Test {metric}: {mean_metric:.4f} (Std: {std_metric:.4f})\")\n",
        "\n",
        "        # --- Train the final model on the full training data ---\n",
        "        # This model will be saved to MLflow artifacts for potential deployment\n",
        "        print(f\"  Fitting final {model_name} pipeline on full training data for logging...\")\n",
        "        full_pipeline.fit(X_full, y_full)\n",
        "\n",
        "        # Log the final trained pipeline\n",
        "        mlflow.sklearn.log_model(full_pipeline, \"final_model_pipeline\")\n",
        "        print(f\"  Final {model_name} pipeline logged to: {mlflow.active_run().info.artifact_uri}/final_model_pipeline\")\n",
        "\n",
        "\n",
        "# --- Main Execution Function ---\n",
        "def main():\n",
        "    # --- 1. Load Data ---\n",
        "    # Check if data files exist\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\") # Create data directory if it doesn't exist\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(f\"Required files ({TRAIN_FILE}, {TEST_FILE}) not found.\")\n",
        "        print(\"Please download them from Kaggle Titanic competition:\")\n",
        "        print(\"https://www.kaggle.com/c/titanic/data\")\n",
        "        print(\"Place 'train.csv' and 'test.csv' inside the 'data/' directory.\")\n",
        "        return # Exit the main function if files are missing\n",
        "\n",
        "    df_train = pd.read_csv(TRAIN_FILE)\n",
        "    df_test = pd.read_csv(TEST_FILE) # This is the \"unseen\" Kaggle test set\n",
        "\n",
        "    # Drop 'PassengerId', 'Name', 'Ticket', 'Cabin' from both train and test data for this example\n",
        "    cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "    df_train_processed = df_train.drop(columns=[col for col in cols_to_drop if col in df_train.columns])\n",
        "    df_test_processed = df_test.drop(columns=[col for col in cols_to_drop if col in df_test.columns]) # Keep this for consistent column dropping, even if not used for prediction in this example\n",
        "\n",
        "    # Define target and features\n",
        "    target = 'Survived'\n",
        "    X_full = df_train_processed.drop(target, axis=1)\n",
        "    y_full = df_train_processed[target]\n",
        "\n",
        "    # Define feature types for preprocessing\n",
        "    numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "    categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "\n",
        "    # --- 2. Define Preprocessing Pipeline (with Cleaning Steps) ---\n",
        "    # Preprocessing for numerical features\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for categorical features\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Create a preprocessor using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop' # Drops any columns not specified in numerical_features or categorical_features\n",
        "    )\n",
        "\n",
        "    # --- Set MLflow Experiment ---\n",
        "    mlflow.set_experiment(\"Titanic CV & Model Logging\")\n",
        "    print(\"Starting Titanic Classification with CV and Model Logging...\")\n",
        "\n",
        "    # --- Define Models and their Parameters ---\n",
        "    models_to_compare = [\n",
        "        {\n",
        "            \"name\": \"Logistic Regression\",\n",
        "            \"classifier\": LogisticRegression(C=0.1, solver='liblinear', max_iter=1000, random_state=42),\n",
        "            \"params\": {\"C\": 0.1, \"solver\": \"liblinear\", \"max_iter\": 1000}\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Random Forest Classifier\",\n",
        "            \"classifier\": RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42),\n",
        "            \"params\": {\"n_estimators\": 100, \"max_depth\": 8}\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Gradient Boosting Classifier\",\n",
        "            \"classifier\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
        "            \"params\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # --- Run the full pipeline for each model ---\n",
        "    for model_info in models_to_compare:\n",
        "        train_evaluate_log_model(\n",
        "            model_info[\"name\"],\n",
        "            model_info[\"classifier\"],\n",
        "            X_full,\n",
        "            y_full,\n",
        "            model_info[\"params\"],\n",
        "            preprocessor, # Pass the preprocessor defined in main()\n",
        "            N_SPLITS_CV   # Pass the N_SPLITS_CV constant\n",
        "        )\n",
        "\n",
        "    print(\"\\nAll Titanic models processed with CV and models logged to MLflow.\")\n",
        "    print(\"To view results, run 'mlflow ui' in your terminal and navigate to the 'Titanic CV & Model Logging' experiment.\")\n",
        "\n",
        "# --- Guard to run main() when script is executed directly ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-single-prediction-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Calling a Logged Model - Single Data Point\n",
        "\n",
        "## Demonstrating Model Loading and Prediction for a Single Entry\n",
        "\n",
        "This section illustrates how to load a previously logged MLflow model and use it to make a prediction for a single new, unseen data point. This is a fundamental step in the model deployment and inference phase of the machine learning lifecycle, often used for real time predictions or individual queries.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary objective is to demonstrate the ease with which MLflow allows you to retrieve and utilize a trained model artifact. Specifically, it covers:\n",
        "\n",
        "1.  **Identifying the Model:** How to specify the exact model to load using its MLflow Run ID and the artifact path under which it was saved.\n",
        "2.  **Constructing the Model URI:** Forming the unique MLflow URI (`runs:/<RUN_ID>/<ARTIFACT_PATH>`) that points directly to the logged model.\n",
        "3.  **Loading the Model:** Using `mlflow.sklearn.load_model()` (or the appropriate flavor-specific `load_model` function) to deserialize the model pipeline into memory.\n",
        "4.  **Making a Single Prediction:** Applying the loaded model to a single new input data row to generate a prediction.\n",
        "5.  **Error Handling:** Including a `try-except` block to gracefully handle scenarios where the model cannot be loaded, providing helpful debugging information.\n",
        "\n",
        "This process is fundamental for integrating MLflow managed models into production systems, especially for scenarios requiring individual predictions.\n",
        "\n",
        "### Key Considerations\n",
        "\n",
        "* **`MLFLOW_RUN_ID`:** This is the unique identifier for the specific MLflow run during which your desired model was trained and logged. You can find this ID in the MLflow UI or from the output of the training script (e.g., from the `mlflow.active_run().info.run_id` printout).\n",
        "* **`MODEL_ARTIFACT_PATH`:** This is the name you gave to the model when it was logged (e.g., `\"final_model_pipeline\"` or `\"decision_tree_classifier_model\"`). It's the sub-path within the run's artifact directory.\n",
        "* **Input Data Format:** The new data (`new_passenger` in this example) used for prediction **must** have the same feature columns and data types as the data used to train the original model. If the original model included a preprocessing pipeline (as in the Titanic example), the loaded object will be the entire pipeline, and you should pass raw, un-preprocessed data to it as a Pandas DataFrame, even for a single row.\n",
        "* **Environment:** Ensure that the Python environment where you are loading the model has all the necessary libraries installed (e.g., `scikit-learn`, `pandas`, `mlflow`) that were used when the model was originally saved.\n",
        "\n",
        "This example uses a hardcoded `MLFLOW_RUN_ID` for demonstration purposes. In a real world scenario, this ID might be dynamically retrieved from a model registry, a database, or a configuration file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "184eebef",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cca715042e7d41d5838789673455c484",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:    0%|           | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "062d8fb94e7149d2b670c9ec8252ae7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:    0%|           | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating loaded model prediction from Run ID: a3a7aff216464fefab633d2962aed98d ---\n",
            "Loaded pipeline from MLflow URI: runs:/a3a7aff216464fefab633d2962aed98d/final_model_pipeline\n",
            "New passenger data:\n",
            "   Pclass   Sex   Age  SibSp  Parch   Fare Embarked\n",
            "0       3  male  28.0      0      0   10.0        S\n",
            "Predicted Survival (0=No, 1=Yes): 0\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Provide the specific Run ID you want to test here.\n",
        "# This should be the ID of a run where your 'final_model_pipeline' was logged.\n",
        "MLFLOW_RUN_ID = \"a3a7aff216464fefab633d2962aed98d\" # <--- Your specified Run ID\n",
        "\n",
        "# Path to the model artifact within the run\n",
        "# This should match the name given when the model was logged (e.g., \"final_model_pipeline\")\n",
        "MODEL_ARTIFACT_PATH = \"final_model_pipeline\"\n",
        "\n",
        "# Construct the MLflow model URI\n",
        "MODEL_URI = f\"runs:/{MLFLOW_RUN_ID}/{MODEL_ARTIFACT_PATH}\"\n",
        "\n",
        "try:\n",
        "    # Load the model directly using the specified MLFLOW_RUN_ID and MODEL_ARTIFACT_PATH\n",
        "    loaded_pipeline = mlflow.sklearn.load_model(MODEL_URI)\n",
        "\n",
        "    print(f\"\\n--- Demonstrating loaded model prediction from Run ID: {MLFLOW_RUN_ID} ---\")\n",
        "    print(f\"Loaded pipeline from MLflow URI: {MODEL_URI}\")\n",
        "\n",
        "    # Create new unseen data (MUST have the same columns as original X)\n",
        "    # Ensure column names match the features used during training and preprocessing\n",
        "    new_passenger = pd.DataFrame([{\n",
        "        'Pclass': 3, 'Sex': 'male', 'Age': 28.0, 'SibSp': 0,\n",
        "        'Parch': 0, 'Fare': 10.0, 'Embarked': 'S'\n",
        "    }])\n",
        "\n",
        "    predicted_survival = loaded_pipeline.predict(new_passenger)[0]\n",
        "    print(f\"New passenger data:\\n{new_passenger}\")\n",
        "    print(f\"Predicted Survival (0=No, 1=Yes): {predicted_survival}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not demonstrate model loading: {e}\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(f\"1. The MLflow run with ID '{MLFLOW_RUN_ID}' exists.\")\n",
        "    print(f\"2. The model artifact '{MODEL_ARTIFACT_PATH}' exists within that run's artifacts.\")\n",
        "    print(\"3. Your 'mlruns' directory is accessible from where you are running this script.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-multiple-predictions-explanation",
      "metadata": {},
      "source": [
        "# MLflow: Calling a Logged Model - Multiple Data Points\n",
        "\n",
        "## Demonstrating Model Loading and Prediction for Multiple Entries\n",
        "\n",
        "This section extends the previous example to show how to load an MLflow model and use it to make predictions for an array or a batch of new, unseen data points. This is a common scenario for batch processing or when serving multiple requests simultaneously.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary objective is to demonstrate how to efficiently use a loaded MLflow model to predict outcomes for multiple inputs. Specifically, it covers:\n",
        "\n",
        "1.  **Re-loading the Model:** The process of loading the model remains the same, emphasizing that the loaded object can handle multiple rows of input.\n",
        "2.  **Preparing Batch Input:** Creating a Pandas DataFrame containing several new data points, ensuring that the structure (column names and types) matches the expectations of the trained model/pipeline.\n",
        "3.  **Batch Prediction:** Applying the `predict()` method of the loaded model to the entire DataFrame of new data, which returns an array of predictions.\n",
        "4.  **Integrating Predictions:** Demonstrating how to add the predictions back into the input DataFrame for easy interpretation and analysis.\n",
        "\n",
        "This capability is essential for scalable inference and integrating MLflow models into data pipelines or applications that process multiple records at once.\n",
        "\n",
        "### Key Considerations\n",
        "\n",
        "* **`MLFLOW_RUN_ID` and `MODEL_ARTIFACT_PATH`:** As before, these must accurately point to the desired logged model.\n",
        "* **Batch Input Format:** When providing multiple data points, they should be structured as rows in a Pandas DataFrame. Each row represents a single instance for which a prediction is desired, and all columns must correspond to the features the model was trained on.\n",
        "* **Output:** The `predict()` method will return an an array of predictions, where each element corresponds to the prediction for the respective row in the input DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c9a643",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7274b1ad2ce2461b94f3d40f76c8c0a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:    0%|           | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b904ec52e7f490196b9208ac48e2a98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:    0%|           | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Demonstrating loaded model prediction from Run ID: a3a7aff216464fefab633d2962aed98d ---\n",
            "Loaded pipeline from MLflow URI: runs:/a3a7aff216464fefab633d2962aed98d/final_model_pipeline\n",
            "New passenger data:\n",
            "   Pclass     Sex   Age  SibSp  Parch   Fare Embarked\n",
            "0       1  female  25.0      1      0   75.0        C\n",
            "1       3    male  35.0      0      0    8.0        S\n",
            "2       2  female  40.0      2      1   45.0        Q\n",
            "Predicted Surval for all passengers (0=No, 1=Yes):\n",
            "[1 0 1]\n",
            "\n",
            "DataFrame with Predictions:\n",
            "   Pclass     Sex   Age  SibSp  Parch   Fare Embarked  Predicted_Survival\n",
            "0       1  female  25.0      1      0   75.0        C                   1\n",
            "1       3    male  35.0      0      0    8.0        S                   0\n",
            "2       2  female  40.0      2      1   45.0        Q                   1\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# --- Configuration for Model Loading ---\n",
        "# IMPORTANT: Replace this with the actual Run ID from your MLflow experiment.\n",
        "# This should be the ID of a run where your 'final_model_pipeline' was logged.\n",
        "MLFLOW_RUN_ID = \"a3a7aff216464fefab633d2962aed98d\" # <--- Your specified Run ID goes here\n",
        "\n",
        "# Path to the model artifact within the specified run's artifacts.\n",
        "# This should match the name given when the model was logged (e.g., \"final_model_pipeline\").\n",
        "MODEL_ARTIFACT_PATH = \"final_model_pipeline\"\n",
        "\n",
        "# Construct the full MLflow model URI. This URI uniquely identifies the model artifact.\n",
        "MODEL_URI = f\"runs:/{MLFLOW_RUN_ID}/{MODEL_ARTIFACT_PATH}\"\n",
        "\n",
        "try:\n",
        "    # Load the model directly using the specified MLflow URI.\n",
        "    # MLflow automatically handles downloading artifacts and reconstructing the model.\n",
        "    loaded_pipeline = mlflow.sklearn.load_model(MODEL_URI)\n",
        "\n",
        "    print(f\"\\n--- Demonstrating loaded model prediction from Run ID: {MLFLOW_RUN_ID} ---\")\n",
        "    print(f\"Loaded pipeline from MLflow URI: {MODEL_URI}\")\n",
        "\n",
        "    # Create new unseen data for prediction. This is a DataFrame with multiple passengers.\n",
        "    # Ensure column names and order match the features used during training and preprocessing.\n",
        "    new_passengers_data = [\n",
        "        # Passenger 1 (Example: likely to survive)\n",
        "        {'Pclass': 1, 'Sex': 'female', 'Age': 25.0, 'SibSp': 1, 'Parch': 0, 'Fare': 75.0, 'Embarked': 'C'},\n",
        "        # Passenger 2 (Example: likely not to survive)\n",
        "        {'Pclass': 3, 'Sex': 'male', 'Age': 35.0, 'SibSp': 0, 'Parch': 0, 'Fare': 8.0, 'Embarked': 'S'},\n",
        "        # Passenger 3 (Example: another likely to survive)\n",
        "        {'Pclass': 2, 'Sex': 'female', 'Age': 40.0, 'SibSp': 2, 'Parch': 1, 'Fare': 45.0, 'Embarked': 'Q'}\n",
        "    ]\n",
        "    new_passengers_df = pd.DataFrame(new_passengers_data)\n",
        "\n",
        "    print(f\"New passenger data:\\n{new_passengers_df}\")\n",
        "\n",
        "    # Make predictions for all passengers in the DataFrame.\n",
        "    predicted_survivals = loaded_pipeline.predict(new_passengers_df)\n",
        "\n",
        "    print(f\"Predicted Survival for all passengers (0=No, 1=Yes):\\n{predicted_survivals}\")\n",
        "\n",
        "    # Optionally, add the predictions back to the DataFrame for easy viewing.\n",
        "    new_passengers_df['Predicted_Survival'] = predicted_survivals\n",
        "    print(\"\\nDataFrame with Predictions:\")\n",
        "    print(new_passengers_df)\n",
        "\n",
        "except Exception as e:\n",
        "    # Catch any exceptions during model loading or prediction and provide informative messages.\n",
        "    print(f\"\\nCould not demonstrate model loading: {e}\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    print(\"Please ensure the following:\")\n",
        "    print(f\"1. The MLflow run with ID '{MLFLOW_RUN_ID}' exists and is accessible.\")\n",
        "    print(f\"2. The model artifact '{MODEL_ARTIFACT_PATH}' exists within that run's artifacts directory.\")\n",
        "    print(\"3. Your 'mlruns' directory (or configured MLflow tracking URI) is accessible from where you are running this script.\")\n",
        "    print(\"4. All necessary Python libraries (e.g., scikit-learn, pandas) are installed in your environment.\")\n",
        "    print(\"5. The input data for prediction has the exact same column names and structure as the training data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fastapi-deployment-explanation",
      "metadata": {},
      "source": [
        "# MLflow: FastAPI Deployment\n",
        "\n",
        "## Deploying a Logged MLflow Model as a Web API with FastAPI\n",
        "\n",
        "This section demonstrates how to deploy a machine learning model, previously logged with MLflow, as a RESTful API using FastAPI. FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The primary goal is to create a simple, runnable web service that can receive new data and return predictions using the MLflow-logged model. This involves:\n",
        "\n",
        "1.  **FastAPI Application Setup:** Initializing a FastAPI application.\n",
        "2.  **Model Loading on Startup:** Implementing an `@app.on_event(\"startup\")` function to load the MLflow model only once when the API starts. This prevents redundant model loading for every request, improving performance.\n",
        "3.  **Input Data Validation with Pydantic:** Defining a `Pydantic` `BaseModel` (`TitanicPassenger`) to strictly validate the structure and types of incoming request data. This ensures that the input data conforms to the features expected by our trained model.\n",
        "4.  **Prediction Endpoint:** Creating a POST endpoint (`/predict`) that accepts passenger data, converts it into a Pandas DataFrame (the format expected by our `scikit-learn` pipeline), uses the loaded MLflow model to make a prediction, and returns the result.\n",
        "5.  **Robust Error Handling:** Including `try-except` blocks to catch potential issues during model loading or prediction, providing informative HTTP error responses.\n",
        "\n",
        "This setup is ideal for integrating your MLflow models into larger applications, microservices architectures, or providing real-time inference capabilities.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "* **`mlflow`:** Used for loading the pre-trained model from its MLflow URI.\n",
        "* **`fastapi`:** The web framework for building the API.\n",
        "* **`pydantic`:** Used by FastAPI for data validation and serialization/deserialization.\n",
        "* **`uvicorn`:** An ASGI server that runs the FastAPI application.\n",
        "* **`pandas`:** For structuring input data into a DataFrame format expected by the `scikit-learn` pipeline.\n",
        "\n",
        "**Before Running:**\n",
        "\n",
        "Make sure you have `mlflow`, `fastapi`, `uvicorn`, and `pandas` installed:\n",
        "```bash\n",
        "pip install mlflow fastapi uvicorn pandas\n",
        "```\n",
        "\n",
        "**Important Note on `MLFLOW_RUN_ID`:**\n",
        "\n",
        "You **must** replace the placeholder `MLFLOW_RUN_ID` with the actual Run ID of one of your trained models from the previous MLflow experiments (e.g., from the Titanic CV & Model Logging experiment). This ID tells FastAPI which specific model to load from your MLflow tracking server or local `mlruns` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa8e81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# app.py - This file defines the FastAPI application for model deployment\n",
        "\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# IMPORTANT: Replace with the actual Run ID of your BEST model from MLflow UI!\n",
        "# This ID links to the specific run where your 'final_model_pipeline' was logged.\n",
        "# Example: MLFLOW_RUN_ID = \"a1b2c3d4e5f67890abcd1234\"\n",
        "MLFLOW_RUN_ID = \"a3a7aff216464fefab633d2962aed98d\" # <--- REPLACE THIS LINE WITH YOUR RUN ID!\n",
        "\n",
        "# Path to the model artifact within the run's artifacts directory.\n",
        "# This should match the name given when the model was logged (e.g., \"final_model_pipeline\").\n",
        "MODEL_ARTIFACT_PATH = \"final_model_pipeline\"\n",
        "\n",
        "# Construct the MLflow model URI. This URI tells MLflow where to find the model.\n",
        "# If using a remote MLflow Tracking Server, you might need to set its URI first:\n",
        "# mlflow.set_tracking_uri(\"http://your_mlflow_server_ip:5000\")\n",
        "MODEL_URI = f\"runs:/{MLFLOW_RUN_ID}/{MODEL_ARTIFACT_PATH}\"\n",
        "\n",
        "# Initialize FastAPI app instance.\n",
        "app = FastAPI(\n",
        "    title=\"Titanic Survival Prediction API\",\n",
        "    description=\"API for predicting survival on the Titanic using an MLflow-logged model.\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Global variable to hold the loaded model pipeline.\n",
        "# It's initialized to None and loaded once on application startup.\n",
        "model_pipeline = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def load_model():\n",
        "    \"\"\"\n",
        "    Load the MLflow model pipeline when the FastAPI application starts up.\n",
        "    This is an asynchronous function that runs once at the beginning.\n",
        "    It ensures the model is loaded only once and is available for all incoming requests,\n",
        "    avoiding redundant loading and improving API performance.\n",
        "    If the model fails to load, it raises an HTTPException, preventing the server from starting.\n",
        "    \"\"\"\n",
        "    global model_pipeline # Declare that we are modifying the global 'model_pipeline' variable\n",
        "    try:\n",
        "        print(f\"Loading model from MLflow URI: {MODEL_URI}\")\n",
        "        # Use mlflow.sklearn.load_model because the model was logged using mlflow.sklearn flavor.\n",
        "        model_pipeline = mlflow.sklearn.load_model(MODEL_URI)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Raise an HTTP exception to indicate a critical startup failure.\n",
        "        raise HTTPException(status_code=500, detail=f\"Model failed to load: {e}. \"\n",
        "                                                      f\"Please check MLFLOW_RUN_ID and ensure MLflow tracking server \"\n",
        "                                                      f\"is running correctly for local runs or configured for remote.\")\n",
        "\n",
        "# Define the input data schema using Pydantic.\n",
        "# This ensures that incoming JSON requests conform to the expected structure and data types.\n",
        "class TitanicPassenger(BaseModel):\n",
        "    Pclass: int\n",
        "    Sex: str\n",
        "    Age: float\n",
        "    SibSp: int\n",
        "    Parch: int\n",
        "    Fare: float\n",
        "    Embarked: str\n",
        "\n",
        "    # Example schema for OpenAPI documentation (Swagger UI).\n",
        "    class Config:\n",
        "        schema_extra = {\n",
        "            \"example\": {\n",
        "                \"Pclass\": 3,\n",
        "                \"Sex\": \"male\",\n",
        "                \"Age\": 28.0,\n",
        "                \"SibSp\": 0,\n",
        "                \"Parch\": 0,\n",
        "                \"Fare\": 10.0,\n",
        "                \"Embarked\": \"S\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Define the prediction endpoint.\n",
        "@app.post(\"/predict\")\n",
        "async def predict_survival(passenger: TitanicPassenger):\n",
        "    \"\"\"\n",
        "    Predicts survival for a single Titanic passenger based on the provided data.\n",
        "    Expects a JSON body conforming to the TitanicPassenger schema.\n",
        "    Returns the predicted survival status (0 or 1) and a human-readable string.\n",
        "    \"\"\"\n",
        "    # Check if the model has been successfully loaded. If not, return a 503 Service Unavailable error.\n",
        "    if model_pipeline is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded yet. Please wait for startup or check logs.\")\n",
        "\n",
        "    # Convert the incoming Pydantic model data into a Pandas DataFrame.\n",
        "    # This is crucial because scikit-learn pipelines typically expect DataFrame input.\n",
        "    # .model_dump() is used for Pydantic v2; for v1, it would be .dict()\n",
        "    input_df = pd.DataFrame([passenger.model_dump()]) \n",
        "\n",
        "    try:\n",
        "        # Make a prediction using the loaded MLflow pipeline.\n",
        "        # The pipeline handles all preprocessing steps internally.\n",
        "        prediction = model_pipeline.predict(input_df)[0]\n",
        "        survival_status = \"Survived\" if prediction == 1 else \"Not Survived\"\n",
        "        \n",
        "        # Return the prediction and a descriptive status.\n",
        "        return {\"prediction\": int(prediction), \"survival_status\": survival_status}\n",
        "    except Exception as e:\n",
        "        # Catch any errors during the prediction process and return a 500 Internal Server Error.\n",
        "        raise HTTPException(status_code=500, detail=f\"Prediction failed: {e}. \"\n",
        "                                                      f\"Ensure input data matches training features and types.\")\n",
        "\n",
        "# You can add a root endpoint for basic health check or info\n",
        "@app.get(\"/\", tags=[\"Health Check\"])\n",
        "async def root():\n",
        "    return {\"message\": \"Titanic Survival Prediction API is running. Visit /docs for API documentation.\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "run-fastapi-instructions",
      "metadata": {},
      "source": [
        "### Run the FastAPI Application\n",
        "\n",
        "To run this FastAPI application:\n",
        "\n",
        "1.  **Save the code:** Save the Python code from the previous cell into a file named `app.py` in your working directory.\n",
        "2.  **Open your terminal:** Navigate to the directory where you saved `app.py`.\n",
        "3.  **Execute the command:** Run the following command:\n",
        "    ```bash\n",
        "    uvicorn app:app --reload\n",
        "    ```\n",
        "    * `app:app` refers to the `app` object within the `app.py` file.\n",
        "    * `--reload` enables auto-reloading, which is useful during development as it restarts the server whenever code changes are detected.\n",
        "\n",
        "You should see output indicating that Uvicorn is running the server, typically on `http://127.0.0.1:8000` or `http://localhost:8000`.\n",
        "\n",
        "### Test Your API\n",
        "\n",
        "Once the server is running, you can test your API using a web browser or a tool like `curl` or Postman:\n",
        "\n",
        "-   **Root Endpoint:** Open your web browser and go to: `http://localhost:8000/`\n",
        "    * You should see a simple JSON response: `{\"message\": \"Titanic Survival Prediction API is running. Visit /docs for API documentation.\"}`\n",
        "\n",
        "-   **Interactive API Documentation (Swagger UI):** Open your web browser and go to: `http://localhost:8000/docs`\n",
        "    * FastAPI automatically generates interactive API documentation (Swagger UI). You can see your `/predict` endpoint, the expected input schema, and example requests. You can use the \"Try it out\" button to send sample requests directly from your browser and see the responses.\n",
        "\n",
        "-   **Alternative Documentation (ReDoc):** For a different documentation style, visit: `http://localhost:8000/redoc`\n",
        "\n",
        "Once the model has been selected and deployed with FastAPI, you can then access it via web clients or programmatically via HTTP requests."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "calling-mlflow-with-request-explanation",
      "metadata": {},
      "source": [
        "### Calling the Deployed MLflow Model with Python `requests`\n",
        "\n",
        "This section demonstrates how to programmatically interact with the FastAPI model deployment using Python's `requests` library. This is how other applications or services would typically consume your deployed machine learning model.\n",
        "\n",
        "### Purpose of This Example\n",
        "\n",
        "The goal is to show how to send a JSON payload to the `/predict` endpoint of your running FastAPI application and process the JSON response. This includes:\n",
        "\n",
        "1.  **Defining the API Endpoint:** Specifying the URL of your FastAPI prediction endpoint.\n",
        "2.  **Preparing Request Headers:** Setting the `Content-Type` header to `application/json` to indicate that the request body is in JSON format.\n",
        "3.  **Creating the Request Body:** Constructing a Python dictionary that matches the `TitanicPassenger` Pydantic schema defined in your `app.py`.\n",
        "4.  **Sending the POST Request:** Using `requests.post()` to send the data to the API.\n",
        "5.  **Handling the Response:** Checking the HTTP status code and parsing the JSON response from the API.\n",
        "\n",
        "This client-side code simulates how a front-end application, another microservice, or a batch processing script would interact with your deployed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "342e1b48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction successful:\n",
            "{'prediction': 0, 'survival_status': 'Not Survived'}\n"
          ]
        }
      ],
      "source": [
        "import requests # Import the requests library for making HTTP requests\n",
        "\n",
        "# --- Configuration for API Call ---\n",
        "url = \"http://localhost:8000/predict\" # The URL of your running FastAPI prediction endpoint\n",
        "headers = {\"Content-Type\": \"application/json\"} # Specify that we are sending JSON data\n",
        "\n",
        "# The data payload for a single passenger, matching the TitanicPassenger Pydantic model.\n",
        "data = {\n",
        "    \"Pclass\": 3,\n",
        "    \"Sex\": \"male\",\n",
        "    \"Age\": 28.0,\n",
        "    \"SibSp\": 0,\n",
        "    \"Parch\": 0,\n",
        "    \"Fare\": 10.0,\n",
        "    \"Embarked\": \"S\"\n",
        "}\n",
        "\n",
        "# Send the POST request to the FastAPI endpoint.\n",
        "# The 'json' parameter automatically serializes the Python dict to JSON and sets the Content-Type header.\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check the HTTP status code of the response.\n",
        "if response.status_code == 200:\n",
        "    print(\"Prediction successful:\")\n",
        "    # Parse the JSON response body and print it.\n",
        "    print(response.json())\n",
        "else:\n",
        "    # If the request was not successful, print the status code and the error message.\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n",
        "    print(\"Please ensure the FastAPI application (app.py) is running and accessible at http://localhost:8000.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4a9967",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Replicate the Churn project using MLFlow. Test different algorithms like Logistic Regression, Random forest and Gradient Boost Classifier, select the best one and deploy it with MLFlow directly and also with FastAPI. Take the Titanic example as a baseline to build your solution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce866e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "##Answer:\n",
        "\n",
        "import os\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema, ColSpec\n",
        "\n",
        "# --- Constants ---\n",
        "N_SPLITS_CV = 5\n",
        "RANDOM_SEED = 42\n",
        "MLFLOW_EXPERIMENT_NAME = \"Telco Churn Prediction CV & Model Logging\"\n",
        "\n",
        "# --- Data Loading and Initial Preprocessing ---\n",
        "def load_data(data_path: str):\n",
        "    \"\"\"\n",
        "    Loads the Telco Customer Churn dataset, performs initial cleaning,\n",
        "    and separates features (X) from the target (y).\n",
        "    \"\"\"\n",
        "    churn = pd.read_csv(data_path)\n",
        "    churn['TotalCharges'] = pd.to_numeric(churn['TotalCharges'], errors='coerce')\n",
        "    churn.dropna(subset=['TotalCharges'], inplace=True)\n",
        "    \n",
        "    y = LabelEncoder().fit_transform(churn['Churn'])\n",
        "    X = churn.drop('Churn', axis=1)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# --- Preprocessing Pipeline Definition ---\n",
        "def define_preprocessor(X: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Defines the preprocessing steps for numerical and categorical features.\n",
        "    \"\"\"\n",
        "    # Identify categorical and numerical features\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical features\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Create a preprocessor using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'  # Keep other columns not specified\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "# --- Model Training, Evaluation, and MLflow Logging Function ---\n",
        "def train_evaluate_log_model(model_name: str, classifier, X_full: pd.DataFrame, y_full: np.ndarray, model_params: dict, preprocessor: ColumnTransformer, n_splits_cv_param: int):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a given model using cross-validation, logs metrics and the model to MLflow.\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=f\"{model_name} Training\"):\n",
        "        print(f\"\\n--- Training {model_name} with Cross-Validation ---\")\n",
        "        mlflow.log_params(model_params)\n",
        "\n",
        "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                   ('classifier', classifier(**model_params))])\n",
        "\n",
        "        # --- Perform Cross-Validation using scikit-learn's cross_validate ---\n",
        "        # This is more concise and less error-prone than a manual loop.\n",
        "        skf = StratifiedKFold(n_splits=n_splits_cv_param, shuffle=True, random_state=RANDOM_SEED)\n",
        "        scoring_metrics = {\n",
        "            'accuracy': 'accuracy',\n",
        "            'precision': 'precision_weighted',\n",
        "            'recall': 'recall_weighted',\n",
        "            'f1': 'f1_weighted',\n",
        "            'roc_auc': 'roc_auc'\n",
        "        }\n",
        "        \n",
        "        cv_results = cross_validate(pipeline, X_full, y_full, cv=skf, scoring=scoring_metrics, return_train_score=False)\n",
        "\n",
        "        # --- Log Mean CV Metrics ---\n",
        "        print(f\"  {model_name} CV Results:\")\n",
        "        for metric_name in scoring_metrics.keys():\n",
        "            # The key in cv_results is 'test_{metric_key_from_scoring}'\n",
        "            scores = cv_results[f'test_{metric_name}']\n",
        "            mean_score = np.mean(scores)\n",
        "            std_score = np.std(scores)\n",
        "            \n",
        "            # Log to MLflow\n",
        "            mlflow.log_metric(f\"mean_cv_{metric_name}\", mean_score)\n",
        "            mlflow.log_metric(f\"std_cv_{metric_name}\", std_score)\n",
        "            \n",
        "            # Print to console\n",
        "            print(f\"    Mean Test {metric_name}: {mean_score:.4f} (Std: {std_score:.4f})\")\n",
        "\n",
        "        # The previous implementation logged individual fold metrics.\n",
        "        # cross_validate does not expose fold-by-fold metrics easily within a single call.\n",
        "        # Logging the mean and std is generally sufficient and cleaner.\n",
        "        \n",
        "        print(f\"  Fitting final {model_name} pipeline on full training data for logging...\")\n",
        "        full_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                         ('classifier', classifier(**model_params))])\n",
        "        full_pipeline.fit(X_full, y_full)\n",
        "\n",
        "        # Infer signature for MLflow logging\n",
        "        example_input = X_full.sample(1, random_state=RANDOM_SEED)\n",
        "        signature = infer_signature(example_input, full_pipeline.predict(example_input))\n",
        "\n",
        "        # Log the final trained pipeline\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=full_pipeline, \n",
        "            artifact_path=\"final_model_pipeline\",\n",
        "            signature=signature,\n",
        "            input_example=example_input\n",
        "        )\n",
        "        print(f\"  Final {model_name} pipeline logged to: {mlflow.active_run().info.artifact_uri}/final_model_pipeline\")\n",
        "\n",
        "# --- Main Execution Function ---\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrates the entire ML pipeline: data loading, preprocessing,\n",
        "    and training/logging multiple models.\n",
        "    \"\"\"\n",
        "    print(\"Starting Telco Churn Classification with CV and Model Logging...\")\n",
        "\n",
        "    # Get the directory where the current script is located\n",
        "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    # --- Set Tracking URI for Model Registry Support ---\n",
        "    # The default file-based store ('./mlruns') does not support the Model Registry.\n",
        "    # To use registry features (like mlflow.register_model), we need a database-backed store.\n",
        "    # Here, we use a local SQLite database file. MLflow will create 'mlflow.db' in the script's directory.\n",
        "    # Artifacts will still be stored in a local 'mlruns' directory by default.\n",
        "    db_path = os.path.join(base_dir, \"mlflow.db\")\n",
        "    mlflow.set_tracking_uri(f\"sqlite:///{db_path}\")\n",
        "    # Set and confirm MLflow experiment\n",
        "    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
        "    exp = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "    if exp is not None:\n",
        "        print(f\"Logging to experiment ID: {exp.experiment_id}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Experiment '{MLFLOW_EXPERIMENT_NAME}' not found or failed to create.\")\n",
        "\n",
        "    # Build the full path to the CSV file\n",
        "    data_path = os.path.join(base_dir, 'churn_data', 'WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "    # Load data\n",
        "    X_full, y_full = load_data(data_path)\n",
        "    print(f\"Data loaded. X_full shape: {X_full.shape}, y_full shape: {y_full.shape}\")\n",
        "\n",
        "    # Define preprocessor\n",
        "    preprocessor = define_preprocessor(X_full)\n",
        "    print(\"Preprocessor defined.\")\n",
        "\n",
        "    # Define models to compare\n",
        "    models_to_compare = [\n",
        "        {\n",
        "            \"name\": \"Logistic Regression\",\n",
        "            \"classifier\": LogisticRegression,\n",
        "            \"params\": {\"solver\": \"liblinear\", \"random_state\": RANDOM_SEED}\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Random Forest Classifier\",\n",
        "            \"classifier\": RandomForestClassifier,\n",
        "            \"params\": {\"n_estimators\": 100, \"random_state\": RANDOM_SEED}\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Gradient Boosting Classifier\",\n",
        "            \"classifier\": GradientBoostingClassifier,\n",
        "            \"params\": {\"n_estimators\": 100, \"random_state\": RANDOM_SEED}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Run the full pipeline for each model\n",
        "    for model_info in models_to_compare:\n",
        "        train_evaluate_log_model(\n",
        "            model_info[\"name\"],\n",
        "            model_info[\"classifier\"],\n",
        "            X_full,\n",
        "            y_full,\n",
        "            model_info[\"params\"],\n",
        "            preprocessor,\n",
        "            N_SPLITS_CV\n",
        "        )\n",
        "\n",
        "    print(\"\\nAll Telco Churn models processed with CV and models logged to MLflow.\")\n",
        "    print(f\"To view results, run 'mlflow ui' in your terminal and navigate to the '{MLFLOW_EXPERIMENT_NAME}' experiment.\")\n",
        "\n",
        "# --- Guard to run main() when script is executed directly ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
