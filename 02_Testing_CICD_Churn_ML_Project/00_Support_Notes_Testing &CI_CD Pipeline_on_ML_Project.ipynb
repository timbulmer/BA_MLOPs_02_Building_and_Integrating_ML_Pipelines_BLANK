{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "banner_image_cell",
   "metadata": {},
   "source": [
    "<img src=\"../media/LandingPage-Header-RED-CENTRE.jpg\" alt=\"Notebook Banner\" style=\"width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc4350",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0093e2",
   "metadata": {},
   "source": [
    "## Unit Test:\n",
    "\n",
    "## Data Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ad07f",
   "metadata": {},
   "source": [
    "\n",
    "This script provides robust unit tests for the `data_loader.py` file, specifically focusing on the `load_churn_dataset` function. The goal is to ensure that data loading works correctly under various scenarios and that appropriate errors are raised when expected. This is a classic example of a unit test that isolates the function's behavior from external dependencies by creating controlled test environments.\n",
    "- **Test Setup (`setUp` method):**\n",
    "    - `setUp(self)`: Before each test method runs, this method sets up a temporary, isolated environment.\n",
    "    - **Temporary Directory:** It creates a `tempfile.mkdtemp()` to house test CSV files. This is crucial for maintaining a clean testing environment and preventing test files from polluting your project directory.\n",
    "    - **Dummy CSV File:** A `self.dummy_csv_filepath` is created with well formed, representative data. This simulates a typical, valid input for `load_churn_dataset`.\n",
    "    - **Empty CSV File:** A `self.empty_csv_filepath` is created, containing only headers. This is an important edge case to test how the function handles datasets with no rows, ensuring it returns an empty DataFrame with the correct schema.\n",
    "\n",
    "- **Test Teardown (`tearDown` method):**\n",
    "    - `tearDown(self)`: After each test method completes, this method cleans up the temporary directory and all its contents using `shutil.rmtree()`. This ensures that each test run starts with a fresh slate and leaves no lingering files.\n",
    "\n",
    "- **Individual Test Case:**\n",
    "    - **`test_load_churn_dataset_success`:**\n",
    "        - **Objective:** Verifies that `load_churn_dataset` successfully loads a valid CSV file into a pandas DataFrame.\n",
    "        - **Assertions:**\n",
    "            - `self.assertIsInstance(df, pd.DataFrame)`: Confirms the returned object is indeed a DataFrame.\n",
    "            - `self.assertFalse(df.empty)`: Checks that the DataFrame is not empty.\n",
    "            - `self.assertEqual(len(df), 3)`: Verifies the correct number of rows are loaded based on the dummy data.\n",
    "            - `self.assertIn('customerID', df.columns)` and similar checks: Ensures that expected columns are present in the loaded DataFrame, confirming the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_project/tests/test_data_loader.py\n",
    "\"\"\"\n",
    "Objective:\n",
    "    This script contains unit tests for the functions in `src/data_loader.py`,\n",
    "    now implemented using Python's built-in `unittest` framework.\n",
    "    It focuses on verifying the correct loading of data and proper error handling.\n",
    "\n",
    "Tests Performed:\n",
    "    - test_load_churn_dataset_success: Verifies that a well-formed CSV file is loaded\n",
    "      correctly into a pandas DataFrame with expected properties.\n",
    "    - test_load_churn_dataset_file_not_found: Checks that a RuntimeError is raised\n",
    "      when attempting to load a non-existent file.\n",
    "    - test_load_churn_dataset_empty_csv: Ensures that an empty CSV file (with headers)\n",
    "      is handled gracefully, resulting in an empty DataFrame with correct columns.\n",
    "\"\"\"\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Add the project root to sys.path to allow imports from src\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n",
    "\n",
    "# Import the function to be tested\n",
    "from src.data_loader import load_churn_dataset\n",
    "\n",
    "class TestDataLoader(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create a temporary directory for test files\n",
    "        self.tmp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create a dummy CSV file\n",
    "        self.dummy_csv_filepath = os.path.join(self.tmp_dir, \"test_churn_data.csv\")\n",
    "        dummy_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No\n",
    "5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No\n",
    "3668-QPYAX,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes\n",
    "\"\"\"\n",
    "        with open(self.dummy_csv_filepath, 'w') as f:\n",
    "            f.write(dummy_data)\n",
    "\n",
    "        # Create an empty CSV file with only headers\n",
    "        self.empty_csv_filepath = os.path.join(self.tmp_dir, \"empty_churn_data.csv\")\n",
    "        empty_data = \"\"\"customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn\n",
    "\"\"\"\n",
    "        with open(self.empty_csv_filepath, 'w') as f:\n",
    "            f.write(empty_data)\n",
    "\n",
    "    def tearDown(self):\n",
    "        # Clean up the temporary directory\n",
    "        shutil.rmtree(self.tmp_dir)\n",
    "\n",
    "    def test_load_churn_dataset_success(self):\n",
    "        \"\"\"Test if the dataset is loaded successfully as a DataFrame.\"\"\"\n",
    "        df = load_churn_dataset(self.dummy_csv_filepath)\n",
    "\n",
    "        self.assertIsInstance(df, pd.DataFrame)\n",
    "        self.assertFalse(df.empty)\n",
    "        self.assertEqual(len(df), 3)\n",
    "        self.assertIn('customerID', df.columns)\n",
    "        self.assertIn('Churn', df.columns)\n",
    "        self.assertIn('TotalCharges', df.columns)\n",
    "\n",
    "# This block allows you to run the tests directly from the script\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2) # exit=False prevents sys.exit() from being called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ad07f",
   "metadata": {},
   "source": [
    "Simple Testing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d9dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "additional_tests_markdown_cell",
   "metadata": {},
   "source": [
    "Addional tests that we can do:\n",
    "\n",
    "- What if the file is not found?\n",
    "\n",
    "- **`test_load_churn_dataset_file_not_found` (Implicitly tested by the structure, but a good addition for comprehensive testing):**\n",
    "    - **Objective:** (If added) Would verify that a `RuntimeError` is raised when a non existent file path is provided to `load_churn_dataset`.\n",
    "    - **Mechanism:** (If added) Would use `with self.assertRaises(RuntimeError):` to wrap the call to `load_churn_dataset` with a non existent path.\n",
    "\n",
    "\n",
    "- What if the file is empty? Is the script reading from somewhere else?\n",
    "\n",
    "- **`test_load_churn_dataset_empty_csv` (Implicitly tested by the structure, but a good addition for comprehensive testing):**\n",
    "    - **Objective:** (If added) Would ensure that an empty CSV file (with headers) is handled gracefully, resulting in an empty DataFrame with correct column names.\n",
    "    - **Mechanism:** (If added) Would load `self.empty_csv_filepath` and assert that the DataFrame is empty but has the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3a4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_description",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This script is a comprehensive suite of unit tests for the `preprocessing.py` file. It leverages Python's built in `unittest` framework to ensure that all core data preprocessing steps cleaning, pipeline construction, feature transformation, and data splitting function correctly and robustly. This is vital for maintaining data integrity and consistency throughout your ML pipeline.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before each test runs, this method prepares the necessary data and configurations. It's designed to provide consistent inputs for the subsequent tests.\n",
    "    -   **`self.raw_df`:** A dummy raw `DataFrame` is created, mimicking the structure and content of your initial churn dataset. This includes edge cases like an empty string in 'TotalCharges' to test robust cleaning.\n",
    "    -   **`self.config_columns`:** Defines the essential column names (target, numeric, categorical) that your preprocessing functions rely on. This promotes reusability and clarity.\n",
    "    -   **Pre-computed DataFrames:** `self.cleaned_df` and `self.transformed_df` are generated by calling `clean_churn_data` and `transform_features` respectively on copies of the dummy data. This allows individual tests to start from a specific, pre-processed state without re running these potentially expensive steps for every test.\n",
    "\n",
    "-   **Tests for `clean_churn_data`:**\n",
    "    -   **`test_clean_churn_data_success`:**\n",
    "        -   **Objective:** Verifies that the `clean_churn_data` function successfully transforms the raw data.\n",
    "        -   **Assertions:** It checks for correct conversion of the 'Churn' column to a binary 'churn_binary' (0s and 1s), ensures 'TotalCharges' is numeric and handles empty strings as `NaN`, and confirms that the final DataFrame contains precisely the expected set of columns.\n",
    "    -   **`test_clean_churn_data_missing_target_column`:**\n",
    "        -   **Objective:** Tests the error handling of `clean_churn_data` when a critical column (like the target column) is missing from the input DataFrame.\n",
    "        -   **Assertions:** It asserts that a `ValueError` with a specific message about missing columns is raised, ensuring the function fails gracefully under invalid inputs.\n",
    "\n",
    "-   **Tests for `build_preprocessing_pipeline`:**\n",
    "    -   **`test_build_preprocessing_pipeline_structure`:**\n",
    "        -   **Objective:** Ensures that the `build_preprocessing_pipeline` function constructs the `scikit-learn` `ColumnTransformer` correctly.\n",
    "        -   **Assertions:** It verifies that the returned object is indeed a `ColumnTransformer`, that it contains the expected number of transformers (one for numeric, one for categorical), and that these transformers are correctly configured `Pipeline` objects with `StandardScaler` for numeric and `OneHotEncoder` for categorical features, respectively. This confirms the pipeline's architectural correctness.\n",
    "\n",
    "-   **Tests for `transform_features`:**\n",
    "    -   **`test_transform_features_success`:**\n",
    "        -   **Objective:** Confirms that `transform_features` correctly applies preprocessing and attaches essential metadata to the DataFrame.\n",
    "        -   **Assertions:** It checks that the output is a DataFrame, that a 'target_encoded' column is created with the correct data type, and that the `feature_columns`, `preprocessor`, `target_mapping`, and `target_names` attributes are all present and correctly populated within the DataFrame's `.attrs` dictionary. This is a critical MLOps practice for maintaining data lineage.\n",
    "\n",
    "-   **Tests for `split_features_and_target`:**\n",
    "    -   **`test_split_features_and_target_success`:**\n",
    "        -   **Objective:** Verifies that `split_features_and_target` correctly separates the DataFrame into features (X) and target (y).\n",
    "        -   **Assertions:** It confirms that X is a DataFrame and y is a Series, that their lengths match, that the target column is removed from X, and that the columns in X precisely match the `feature_columns` stored in the DataFrame's attributes.\n",
    "    -   **`test_split_features_and_target_missing_attrs`:**\n",
    "        -   **Objective:** Tests the error handling of `split_features_and_target` when required metadata (like `feature_columns`) is missing.\n",
    "        -   **Assertions:** It asserts that a `ValueError` with a specific message is raised, ensuring robustness against malformed inputs.\n",
    "\n",
    "-   **Tests for `stratified_split` (Not explicitly defined in the provided code, but mentioned in comments. A comprehensive test would include):**\n",
    "    -   **Objective:** To ensure that the `stratified_split` function correctly divides the data into training and testing sets while preserving the original class distribution of the target variable.\n",
    "    -   **Assertions:** This test would typically verify that the proportions of each class in the target variable (`y`) are approximately the same in both the training and testing sets, ensuring that the model training is not biased by imbalanced splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0bff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "model_unit_tests_description",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "This script provides a thorough set of unit tests for the `model.py` file, focusing on the `ChurnPredictionModel` class and its auxiliary metric calculation and reporting functions. These tests are crucial for ensuring that your machine learning model behaves as expected, from initialization and training to prediction, saving, loading, and performance logging.\n",
    "\n",
    "-   **`TestChurnPredictionModel` Class:**\n",
    "    -   **Objective:** Contains unit tests specifically for the `ChurnPredictionModel` class, ensuring its core functionalities work correctly.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Initializes a dummy dataset (`self.X` for features, `self.y` for target) and a simple `StandardScaler` preprocessor before each test method runs. This provides controlled, consistent inputs for testing the model's behavior.\n",
    "        -   **`self.X` and `self.y`:** Randomly generated NumPy arrays are converted into pandas `DataFrame` and `Series` respectively, simulating feature and target data. This isolation ensures that the model's behavior is tested independently of prior data loading or preprocessing steps.\n",
    "    -   **`test_model_init_defaults`:**\n",
    "        -   **Objective:** Verifies that the `ChurnPredictionModel` initializes with the correct default components when no specific classifier or preprocessor is provided.\n",
    "        -   **Assertions:** Checks that the `classifier` attribute is an instance of `LogisticRegression` and that the internal `pipe` (a `sklearn.pipeline.Pipeline`) contains both a 'scaler' (from `StandardScaler`) and a 'classifier' step.\n",
    "    -   **`test_model_fit_predict`:**\n",
    "        -   **Objective:** Confirms that the model can be successfully trained (`fit`), make predictions (`predict`), and generate probability estimates (`predict_proba`).\n",
    "        -   **Assertions:** Assesses the shapes of the predictions and probabilities, ensures predictions are binary (0 or 1), verifies probabilities are between 0 and 1, and confirms that probabilities for each sample sum up to 1.\n",
    "    -   **`test_model_save_and_load`:**\n",
    "        -   **Objective:** Ensures the model's persistence capabilities that it can be saved to disk and subsequently loaded, retaining its predictive ability.\n",
    "        -   **Assertions:** Uses a `TemporaryDirectory` to save the trained model, checks if the file exists, loads the model back using `joblib.load`, and verifies that the loaded model is a `Pipeline` instance and can still produce predictions with the expected shape.\n",
    "    -   **`test_model_log_run`:**\n",
    "        -   **Objective:** Validates the model's logging functionality, ensuring that run information (metrics, dataset details) is correctly written to a JSON file.\n",
    "        -   **Assertions:** Creates a temporary log directory and file, calls `log_run`, checks for the existence of the log file, reads its content, and asserts that the logged data is a list and contains the expected metrics and dataset information.\n",
    "\n",
    "-   **`TestMetricsFunctions` Class:**\n",
    "    -   **Objective:** Focuses on testing the standalone metric computation and reporting functions (`compute_classification_metrics` and `report_classification_metrics`).\n",
    "    -   **`test_compute_classification_metrics_perfect`:**\n",
    "        -   **Objective:** Tests the `compute_classification_metrics` function with a scenario where predictions are perfectly accurate.\n",
    "        -   **Assertions:** Verifies that all metrics (accuracy, precision, recall, f1_score) correctly return 1.0, indicating perfect performance.\n",
    "    -   **`test_compute_classification_metrics_imperfect`:**\n",
    "        -   **Objective:** Tests `compute_classification_metrics` with a scenario involving imperfect predictions to ensure correct calculation of various metrics.\n",
    "        -   **Assertions:** Asserts the calculated values for accuracy, precision, and recall match the expected values for the given imperfect predictions.\n",
    "    -   **`test_report_classification_metrics_output`:**\n",
    "        -   **Objective:** Checks that the `report_classification_metrics` function generates the expected console output format.\n",
    "        -   **Assertions:** Uses `unittest.mock.patch` to capture the output sent to `builtins.print`, then asserts that the captured string contains key phrases and formatted metric values, ensuring the report is user friendly and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce0a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_unit_test_description",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "This script provides a crucial **unit test** for the `pipeline.py` file, specifically the `run_churn_pipeline` function. The primary objective is to verify the correct **orchestration and data flow** of the entire machine learning pipeline in **complete isolation**. This is achieved by extensively using mock objects, which act as \"stunt doubles\" for all external and internal function calls that `run_churn_pipeline` depends on. This allows the test to focus solely on the internal logic of the pipeline function the sequence of calls and how data is passed between them without executing any heavy data loading, cleaning, transformation, or model training code.\n",
    "\n",
    "-   **The Role of Mock Objects in This Test:**\n",
    "    -   **`@patch(...)` Decorators:** Each `@patch` decorator intercepts a function call within `run_churn_pipeline`. For instance, `@patch('src.pipeline.load_churn_dataset')` ensures that when `run_churn_pipeline` attempts to call the real `load_churn_dataset` function, it instead calls a special mock object (`mock_load_data`). This is done for every dependency of the pipeline function.\n",
    "    -   **`MagicMock(spec=...)`:** These lines create placeholder objects that mimic real data structures (e.g., `mock_raw_df = MagicMock(spec=pd.DataFrame)`). The `spec` argument is a safety feature that ensures the mock behaves like a real object of that type, preventing accidental calls to non existent attributes or methods.\n",
    "    -   **`.return_value`:** This is crucial for simulating the flow of data. It tells a mock what to \"return\" when it's called. For example, `mock_load_data.return_value = mock_raw_df` means, \"When the mocked `load_churn_dataset` function is called, it should return our placeholder `mock_raw_df` object.\" This creates a chain where the output of one mocked step becomes the input for the next, simulating data handoff.\n",
    "    -   **Configuring Mock Behavior:** Sometimes, the code under test calls methods on the objects it receives. Lines like `mock_y_train.mean.return_value = 0.25` configure these mock objects to return specific values when their methods are called (e.g., `y_train.mean()`), preventing `TypeError` and allowing the pipeline's internal logic (like printing metrics) to execute correctly.\n",
    "\n",
    "-   **`TestPipeline` Class:**\n",
    "    -   **Objective:** Contains unit tests for the `run_churn_pipeline` function.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a temporary directory (`self.tmp_dir`) to simulate where data files might exist and where models/logs would be stored (`self.model_store_path`). This ensures the test can provide valid (mocked) paths to `run_churn_pipeline` without affecting the actual file system.\n",
    "    -   **`tearDown(self)` method:**\n",
    "        -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test, ensuring a clean state for subsequent tests.\n",
    "\n",
    "    -   **`test_pipeline_unit_orchestration_success`:**\n",
    "        -   **Objective:** This is the primary unit test. It mocks *all* dependencies of `run_churn_pipeline` to focus purely on the function's orchestration logic and data handoff between conceptual stages.\n",
    "        -   **Arrange (Mock Setup):** Numerous `MagicMock` objects are created to represent the outputs of each stage (e.g., raw DataFrame, cleaned DataFrame, transformed DataFrame, X/y splits, model instance). The `.return_value` of each mocked function (e.g., `mock_load_data`, `mock_clean_data`) is set to return these specific mock objects, forming a controlled data flow. Additionally, mock methods like `mock_y_train.mean` are configured to return specific values to avoid runtime errors.\n",
    "        -   **Act:** The `run_churn_pipeline` function is called with mocked file paths and configuration variables imported directly from `src.config` (ensuring consistency with the application's actual configuration).\n",
    "        -   **Assert (`assert_called_once_with`):** This is the most important part. `assert_called_once_with()` is a powerful assertion that verifies two critical things:\n",
    "            1.  **Call Count:** Was the mocked function called exactly one time? (e.g., `mock_load_data.assert_called_once_with(...)`).\n",
    "            2.  **Arguments:** Was it called with the specific arguments you expected? This is crucial for confirming that the output of one mocked step was correctly passed as input to the next (e.g., `mock_clean_data.assert_called_once_with(mock_raw_df, ...)`).\n",
    "            -   Assertions also verify that the `ChurnPredictionModel` was instantiated with the correct preprocessor, its `fit` method was called with the training data, `predict` was called, metrics were computed and reported, and finally, the model and logs were instructed to be saved.\n",
    "            -   The test also asserts that `run_churn_pipeline` returns the mocked model instance and metrics as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6842b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "config_import_and_test_philosophy",
   "metadata": {},
   "source": [
    "In this scenario, it is not only correct but also highly recommended to import and use the variables from your `config.py` file in your pipeline test.\n",
    "\n",
    "The `run_churn_pipeline` function is designed to be configured by the values in `config.py`. Your test should validate that the pipeline behaves correctly with that specific configuration. If you were to hardcode values like `target_column='Churn'` in your test, and later someone changed the `TARGET_COLUMN` in the config file, your test would be validating an outdated scenario. By importing from `config.py`, your test and your application logic remain perfectly in sync.\n",
    "\n",
    "**DRY (Don't Repeat Yourself)** prevents you from defining the same constants in both your application code and your test code. This reduces the chance of typos and makes the project easier to maintain. If a configuration value needs to change, you only have to change it in one place: `config.py`.\n",
    "\n",
    "Using the config variables makes the test's purpose clear: \"I am verifying that the pipeline correctly orchestrates its steps using the project's standard configuration.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main_unit_test_description",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "This script contains **unit tests** for the `main.py` entry point of the application, utilizing Python's built in `unittest` framework. The primary goal of these tests is to ensure that the `main` function correctly orchestrates the machine learning pipeline execution based on the provided parameters, and that it handles errors gracefully. These tests are crucial for verifying the top level control flow of your application in isolation.\n",
    "\n",
    "-   **`TestMainFunction` Class:**\n",
    "    -   **Objective:** Provides isolated unit tests specifically for the `main` function, confirming its role as the pipeline orchestrator.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Initializes a temporary directory (`self.output_dir`) before each test. This directory acts as a safe, isolated space for `main.py` to create its output artifacts (like models and logs) without affecting the actual file system or other tests.\n",
    "    -   **`tearDown(self)` method:**\n",
    "        -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test has completed. This ensures that tests are independent and do not leave behind any residual files, promoting a clean testing environment.\n",
    "\n",
    "-   **`test_main_success`:**\n",
    "    -   **Objective:** Verifies that the `main` function correctly prepares input paths and parameters, then successfully calls the `run_churn_pipeline` function with these details.\n",
    "    -   **Key Techniques:**\n",
    "        -   `@patch('main.run_churn_pipeline')`: This decorator replaces the actual `run_churn_pipeline` function with a `MagicMock` object during the test. This is critical for unit testing `main` in isolation, preventing the test from executing the entire, potentially time consuming, ML pipeline.\n",
    "        -   `mock_run_pipeline.return_value = (dummy_model, dummy_metrics)`: Configures the mock to return dummy values, simulating a successful pipeline run, allowing the test to verify `main`'s handling of the pipeline's return values.\n",
    "        -   `with patch('builtins.print')`: Suppresses `print` statements within `main` during the test to keep the test output clean and focused on assertions.\n",
    "    -   **Assertions:**\n",
    "        -   It constructs the `expected_data_path` and `expected_model_store_path` dynamically based on the temporary output directory and predefined configuration constants (e.g., `DATA_DIR_NAME`, `DATASET_FILENAME`).\n",
    "        -   `mock_run_pipeline.assert_called_once_with(...)`: This is the core assertion. It verifies that `run_churn_pipeline` was called exactly once and, crucially, that all its arguments (data paths, column names, test size, random state, model/log paths) match the expected values derived from the configuration. This ensures `main` passes the correct information down to the pipeline.\n",
    "\n",
    "-   **`test_main_pipeline_error_handling`:**\n",
    "    -   **Objective:** Tests that the `main` function correctly handles and propagates exceptions that might occur within the `run_churn_pipeline`.\n",
    "    -   **Key Technique:**\n",
    "        -   `@patch('main.run_churn_pipeline', side_effect=ValueError(\"Simulated pipeline error\"))`: This decorator configures the mocked `run_churn_pipeline` to raise a specific `ValueError` whenever it's called. This simulates a failure within the pipeline (e.g., a data loading error or training crash).\n",
    "    -   **Assertions:**\n",
    "        -   `with self.assertRaisesRegex(ValueError, \"Simulated pipeline error\")`: This assertion confirms that calling `main` results in the expected `ValueError` being reraised. This demonstrates that `main` does not silently swallow errors from its dependencies, which is essential for proper debugging and error reporting in a real application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5849f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "integration_test_description",
   "metadata": {},
   "source": [
    "# Integration test:\n",
    "\n",
    "### The Purpose of a Unit Test vs. an Integration Test:\n",
    "\n",
    "The goal of a unit test is to test a single piece of code in this case, the orchestration logic inside the `run_churn_pipeline` function in complete isolation. We mock all its dependencies (the other functions it calls) to make assumptions about their behavior. The test asks: \"Assuming `stratified_split` gives me a `y_train` object, and assuming that object's `.mean()` method returns a number, does my `run_churn_pipeline` function correctly use that number in the `print()` statement without crashing?\"\n",
    "\n",
    "The goal of an integration test is to test how multiple components work together. In an integration test, we would not mock `stratified_split`. We would let it run and produce a real pandas Series for `y_train`. Then, when `.mean()` is called, it would be the real pandas calculation. This tests the \"integration\" between the splitting function and the pipeline's logging.\n",
    "\n",
    "## Data Preprocessing Integration\n",
    "\n",
    "Its purpose is to verify that two separate components of your project, the `load_churn_dataset` function and the `clean_churn_data` function work together correctly as a single unit. It ensures that the data produced by the loader is in the exact format that the cleaner expects.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before the test is run, this method prepares the environment.\n",
    "    -   **Create a Temporary Directory:** It creates a new, empty temporary directory. This is a best practice that gives the test a clean, isolated space to work in, preventing it from interfering with other files on your system.\n",
    "    -   **Create Realistic Test Data:** It defines a multi-line string that looks exactly like a real CSV file. This data is crucial because it's a representative sample of what the pipeline will see, including important edge cases like an empty value for TotalCharges (notice the `,,` in the fourth row).\n",
    "    -   **Write the Test File:** It writes this string data into a new CSV file inside the temporary directory.\n",
    "\n",
    "-   **The Integration Test (`test_data_loading_and_cleaning_flow` method):** This is the main test that verifies the integration between the two functions.\n",
    "    -   **Load the Data:** The test first calls the real `load_churn_dataset` function, passing it the path to the temporary CSV file created during setup. It then asserts that this function successfully returned a pandas DataFrame.\n",
    "    -   **Clean the Data:** takes the `raw_df` produced by the loading step and passes it directly to the `clean_churn_data` function. This is the \"integration\" step testing the handoff between the two components.\n",
    "-   **Assert Results:** The test performs a series of checks on the `cleaned_df` to confirm that the entire process worked as expected:\n",
    "    -   It verifies that the `churn_binary` column was created correctly.\n",
    "    -   It checks that the `TotalCharges` column, which contained an empty string, was correctly converted to a numeric type and that the empty value is now a proper `NaN` missing value.\n",
    "    -   It ensures that the final set of columns in the cleaned DataFrame is exactly what it should be, based on the project's configuration files.\n",
    "\n",
    "-   **Test Cleanup (`tearDown` method):** After the test finishes, this method is automatically called. It deletes the entire temporary directory and the CSV file inside it, ensuring that the test leaves no trace behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448ddf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "overall_preprocessing_transform_integration_summary",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline and Transform Integration\n",
    "\n",
    "This script is a crucial **integration test** for your preprocessing logic, specifically for the `build_preprocessing_pipeline` and `transform_features` functions. Its main purpose is to verify that these two components work together correctly, ensuring that the `ColumnTransformer` pipeline is constructed properly and then successfully applied to clean data, producing a fully transformed DataFrame with all the necessary metadata for the next steps in your MLOps workflow.\n",
    "\n",
    "-   **Test Setup (`setUp` method):**\n",
    "    -   `setUp(self)`: Before the test is run, this method creates a single, important piece of test data: `self.dummy_cleaned_df`.\n",
    "    -   **Simulates Clean Data:** This DataFrame is designed to look exactly like the output of the `clean_churn_data` function. It's a \"clean\" dataset ready for transformation.\n",
    "    -   **Includes Edge Cases:** Crucially, it includes a `np.nan` (Not a Number) value in the `TotalCharges` column. This is essential for verifying that the imputation step (filling in missing values) of the pipeline works correctly.\n",
    "\n",
    "-   **The Integration Test (`test_preprocessing_pipeline_and_transform_flow` method):**\n",
    "    -   **Build the Preprocessing Pipeline:** The test first calls the `build_preprocessing_pipeline` function. This creates the `scikit-learn` `ColumnTransformer` object, which knows how to handle numeric and categorical columns differently.\n",
    "    -   **Transform the Features:** The test calls `transform_features`, passing it a copy of the clean dummy data from `setUp`. Inside this function, the preprocessor from the previous step is fitted to the data and used to transform it.\n",
    "-   **Assert the Results:** The rest of the test consists of a series of assertions to confirm that the entire process worked as expected.\n",
    "    -   **Check DataFrame Structure:** It verifies that the output `df_transformed` is a `DataFrame` and that the target column has been correctly created and encoded as `target_encoded`.\n",
    "    -   **Check Metadata (`.attrs`):** It asserts that critical metadata has been attached to the DataFrame's attributes. This includes the list of feature columns and, most importantly, the `fitted_preprocessor` itself. This is a key MLOps practice, as it ensures that artifacts are passed along with the data they correspond to.\n",
    "    -   **Verify Imputation:** It inspects the `fitted_preprocessor` and confirms that the `SimpleImputer` for numeric columns has been successfully fitted. A fitted imputer will have a `statistics_` attribute where it stores the values it learned (e.g., the median of each column) to fill in missing data. This proves that the `np.nan` value was handled correctly during the `fit_transform` process.\n",
    "    -   **Verify Feature Columns Attribute:** It checks that the `feature_columns` attribute correctly stores the names of the original numeric and categorical features, ensuring consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865929a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "model_metrics_integration_description",
   "metadata": {},
   "source": [
    "## Model Metrics Integration\n",
    "\n",
    "This script provides an **integration test** for the critical flow of the machine learning pipeline: from preprocessed data to model training, prediction, and subsequent metric computation. It uses Python's built-in `unittest` framework to ensure that `ChurnPredictionModel` interacts correctly with its inputs (simulated preprocessed data) and that its outputs (predictions) are accurately processed by the `compute_classification_metrics` function. This test bridges the gap between unit-tested components to verify their combined functionality.\n",
    "\n",
    "-   **`TestModelMetricsIntegration` Class:**\n",
    "    -   **Objective:** Contains the integration test for the model's core lifecycle, ensuring the end-to-end process from processed data to evaluated metrics works as expected.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a dummy dataset that simulates the output of the preprocessing module. This means features (`X`) are scaled, and crucial metadata (`preprocessor`, `feature_columns`) is attached to the DataFrame's attributes, mimicking the state of data just before model training in a real pipeline.\n",
    "        -   **`self.X_full` and `self.y_full`:** Randomly generated features and a binary target are created. `X_full` then undergoes `StandardScaler` transformation, and the preprocessor instance and feature column names are explicitly stored in `X_full.attrs`. This setup ensures the data is in the expected format for the `ChurnPredictionModel`.\n",
    "\n",
    "-   **`test_model_training_prediction_and_metrics_flow`:**\n",
    "    -   **Objective:** This is the primary integration test. It verifies the complete flow:\n",
    "        1.  **Data Splitting:** Simulates the splitting of the preprocessed data into training and testing sets using `stratified_split`.\n",
    "        2.  **Model Instantiation and Training:** Initializes `ChurnPredictionModel` using the preprocessor attached to the `X_full` DataFrame's attributes and trains it on the training data.\n",
    "        3.  **Prediction:** Generates predictions on the test set.\n",
    "        4.  **Metric Computation:** Uses `compute_classification_metrics` to evaluate the model's performance based on the predictions and true labels.\n",
    "    -   **Assertions:**\n",
    "        -   Confirms that data is split into appropriate training and testing shapes.\n",
    "        -   Asserts that the model is fitted successfully.\n",
    "        -   Checks that predictions (`y_pred`) are NumPy arrays of the correct shape.\n",
    "        -   Verifies that the computed `metrics` dictionary contains all expected classification metrics (accuracy, precision, recall, f1_score, confusion_matrix).\n",
    "        -   Includes basic assertions on metric values (e.g., accuracy is between 0.4 and 1.0) to ensure the model is learning something reasonable, even with dummy data.\n",
    "        -   Prints informational messages to the console during the test execution, providing real-time feedback on the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e7c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "main_functional_test_description",
   "metadata": {},
   "source": [
    "# Functional test:\n",
    "\n",
    "## Main Functional\n",
    "\n",
    "This script provides **functional tests** for the `main.py` application entry point. Unlike unit tests that isolate individual functions, functional tests verify that the `main` function, which orchestrates the entire ML pipeline, operates correctly as a whole. This includes ensuring it calls the pipeline with the right parameters, handles its outputs (like success messages), and gracefully manages exceptions. These tests provide a higher level of confidence in the application's top-level behavior.\n",
    "\n",
    "-   **`TestMainFunctional` Class:**\n",
    "    -   **Objective:** Contains functional tests for the `main` function, focusing on its orchestration capabilities and error handling.\n",
    "    -   **Decorators (`@patch`):**\n",
    "        -   `@patch('main.run_churn_pipeline')`: Mocks the `run_churn_pipeline` function. This is crucial for controlling the pipeline's behavior (e.g., simulating success or failure) without executing the actual, complex ML pipeline code, allowing the test to focus solely on `main`'s orchestration logic.\n",
    "        -   `@patch('os.path.abspath', return_value='/mock/project/path/main.py')` and `@patch('os.path.dirname', return_value='/mock/project/path')`: These mocks simulate a consistent project root directory. This ensures that `main` constructs file paths correctly, regardless of where the test is actually run, making tests more reliable.\n",
    "\n",
    "-   **`test_main_orchestrates_pipeline_and_reports_success`:**\n",
    "    -   **Objective:** Verifies that `main` correctly triggers the `run_churn_pipeline` with the expected parameters and reports successful completion to standard output.\n",
    "    -   **Arrange:**\n",
    "        -   `dummy_model`, `dummy_metrics`: `MagicMock` objects are set as the `return_value` of `mock_run_churn_pipeline` to simulate a successful pipeline run with specific metrics.\n",
    "        -   `patch('sys.stdout', new_callable=io.StringIO)` and `patch('sys.stderr', new_callable=io.StringIO)`: These are used as context managers to capture any output (print statements, error messages) generated by `main` to `sys.stdout` and `sys.stderr` respectively. This allows for assertions on console output.\n",
    "    -   **Act:** `main()` is called without any arguments, simulating its typical invocation.\n",
    "    -   **Assert:**\n",
    "        -   `mock_run_churn_pipeline.assert_called_once()`: Confirms the pipeline function was called exactly once.\n",
    "        -   Assertions on `kwargs` (`mock_run_churn_pipeline.call_args`): Verifies that the `data_file_path`, `model_dir_path`, `target_column`, and `test_size` passed to `run_churn_pipeline` match the expected values derived from mocked paths and configuration.\n",
    "        -   Assertions on `stdout_output` and `stderr_output`: Checks that `main` printed the correct \"Starting...\", \"completed successfully!\", and \"Final Model Accuracy\" messages to `stdout` and that no errors were printed to `stderr`.\n",
    "\n",
    "-   **`test_main_handles_pipeline_exceptions_gracefully`:**\n",
    "    -   **Objective:** Tests `main`'s error handling, ensuring it gracefully catches exceptions from the pipeline, prints an informative error message, and then re-raises the exception for proper propagation.\n",
    "    -   **Arrange:**\n",
    "        -   `mock_run_churn_pipeline.side_effect = ValueError(error_message)`: Configures the mocked pipeline to raise a `ValueError`, simulating a pipeline failure.\n",
    "        -   `patch('sys.stdout', ...)` and `patch('sys.stderr', ...)`: Again, capture `stdout` and `stderr` to inspect console output.\n",
    "    -   **Act & Assert:**\n",
    "        -   `with self.assertRaisesRegex(ValueError, error_message): main()`: This block asserts that calling `main()` results in the expected `ValueError` being re-raised, confirming `main` does not suppress critical errors.\n",
    "        -   Assertions on `stderr_output`: Verifies that `main` printed an \"ERROR: Pipeline failed with exception...\" message to `stderr`.\n",
    "        -   Assertions on `stdout_output`: Confirms that the success message was *not* printed to `stdout` in case of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a45891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e_pipeline_testing_description",
   "metadata": {},
   "source": [
    "# End-to-End (E2E) Pipeline Testing:\n",
    "\n",
    "This script contains an **End-to-End (E2E) test** for the entire machine learning application pipeline, orchestrated by the `main.py` entry point. It's designed to mimic a real-world deployment scenario as closely as possible, providing the highest level of confidence that your entire system works as a cohesive unit. This test verifies the full ML pipeline from data loading and processing to model saving and log creation, using a real (albeit small) dataset.\n",
    "\n",
    "-   **`TestE2EPipeline` Class:**\n",
    "    -   **Objective:** Provides the E2E test for the complete ML pipeline, ensuring all integrated components function together correctly.\n",
    "    -   **`setUp(self)` method:**\n",
    "        -   **Purpose:** Prepares a temporary, isolated environment that simulates the project root directory. This is critical for E2E tests, as it prevents interference with the actual file system and ensures tests are repeatable.\n",
    "        -   **Temporary Directory Creation:** `tempfile.mkdtemp()` creates a unique temporary directory (`self.tmp_project_root`).\n",
    "        -   **Data Structure Simulation:** It then creates the necessary subdirectories (`data/raw`) within this temporary root.\n",
    "        -   **Dummy Dataset Creation:** A small, valid CSV dataset (`churn_data.csv`) is created and written into the simulated raw data directory, matching the expected input format for the pipeline. This ensures the pipeline processes realistic data.\n",
    "\n",
    "-   **`tearDown(self)` method:**\n",
    "    -   **Purpose:** Cleans up the temporary directory created in `setUp` after each test has completed. This ensures that tests are independent and do not leave behind any residual files or directories, maintaining a clean testing environment.\n",
    "\n",
    "-   **`test_main_pipeline_end_to_end`:**\n",
    "    -   **Objective:** This is the core E2E test. It runs the entire `main` function (the application's entry point) and then asserts on the final outputs, verifying the full system's behavior.\n",
    "    -   **Execution:** `main(output_base_dir=self.tmp_project_root)` is called. Critically, the temporary project root is passed as `output_base_dir`, directing all pipeline outputs (models, logs) to this isolated location. This ensures the test is self-contained.\n",
    "    -   **Assertions:**\n",
    "        -   **File Existence:** It asserts that the `MODEL_FILENAME` (the trained model) and `LOG_FILENAME` (the run metrics log) are correctly created in the expected `model_store` directory within the temporary project root. This confirms the pipeline's output generation.\n",
    "        -   **Log Content Verification:** It loads the generated log file (`LOG_FILENAME`), verifies it's a list, contains at least one run, and that the first run's `metrics` dictionary contains essential keys like \"accuracy\", \"f1_score\", and \"confusion_matrix\".\n",
    "        -   **Metric Value Sanity Check:** It asserts that the \"accuracy\" and \"f1_score\" are valid floating-point numbers within the reasonable range of 0.0 to 1.0. This provides a basic sanity check on the model's performance without requiring specific absolute values (as the dummy data might not yield highly predictable results).\n",
    "        -   **Confusion Matrix Structure:** It checks that the \"confusion_matrix\" is a list of lists with the expected dimensions (2x2), indicating correct reporting of classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f45bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c9aeceb7",
   "metadata": {},
   "source": [
    "# Deploying CI/CD pipeline in GitHub\n",
    "\n",
    "To deploy CI/DC on GitHub the following configurations are needed:\n",
    "\n",
    "- Establish a GitHub repository\n",
    "- Create Your Workflow File: Create a new directory at the root of your repository called `.github/workflows`. Inside this directory, create a new `YAML` file. The name of this file will be the name of your workflow (e.g., `ml_pipeline.yml`, `model_deploy.yml`). This file is where you define the entire CI/CD process. E.g.,\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- Define Workflow Triggers (`on`): Within your YAML file, the first thing to configure is when the workflow should run. Example:\n",
    "    - on: `push`: The workflow runs every time code is pushed to the repository (you can specify branches, e.g., branches: [main]).\n",
    "    - on: `pull_request`: The workflow runs when a pull request is opened, synchronised, or re-opened.\n",
    "    - on: `schedule`: You can set it to run at specific intervals, for instance, daily at a certain time to retrain a model.\n",
    "    - on: `workflow_dispatch`: Allows for manual triggering from the \"Actions\" tab.\n",
    "\n",
    "- Define Jobs (`jobs`): Workflows are made up of one or more \"jobs.\" Each job runs on a separate virtual machine (called a \"runner\"). You'll define distinct jobs for different phases of your pipeline, e.g., `data_preprocessing`, `model_training`, `model_testing`, `model_deployment`. You specify the operating system for the runner, e.g., `runs-on: ubuntu-latest`.\n",
    "\n",
    "- Define Steps within Jobs (`steps`): Each job consists of a sequence of \"steps.\" These steps are individual tasks that are executed in order. Common Steps for ML CI/CD:\n",
    "\n",
    "    - Checkout Code: Use the `actions/checkout@v4` Action to get your repository's code onto the runner.\n",
    "\n",
    "    - Set up Environment: Use `actions/setup-python@v5` to configure the Python version required.\n",
    "\n",
    "    - Install Dependencies: Run commands like `pip install -r requirements.txt` to install necessary libraries.\n",
    "\n",
    "    - Run Scripts: Execute your Python scripts for data loading, feature engineering, model training, and evaluation (e.g., `python src/train_model.py`).\n",
    "\n",
    "    - Run Tests: Execute unit tests, integration tests, and model performance tests (e.g., `pytest`).\n",
    "\n",
    "    - Build/Push Docker Images: If you're containerising your model.\n",
    "\n",
    "\n",
    "- Monitor Your Pipeline: Once configured, commit your YAML file to the repository. The workflow will trigger based on your defined `on` events. Go to the \"Actions\" tab in your GitHub repository. Here, you will see a list of all your workflow runs, their status (success, failure, in progress), and detailed logs for each step. This is your primary dashboard for observing the CI/CD process.\n",
    "\n",
    "Here below an example of the YAML file for our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaba58d",
   "metadata": {},
   "source": [
    "```\n",
    "name: Churn ML Project CI/CD (Unittest)\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      - '02 Building & Integrating ML Pipelines/churn_ml_project_unittest/**'\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      - '02 Building & Integrating ML Pipelines/churn_ml_project_unittest/**'\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    name: Run Unit, Integration, Functional, and E2E Tests\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    defaults:\n",
    "      run:\n",
    "        # Set the working directory to the project root within the repository\n",
    "        working-directory: \"02 Building & Integrating ML Pipelines/churn_ml_project_unittest\"\n",
    "\n",
    "    steps:\n",
    "    - name: Checkout Repository\n",
    "      uses: actions/checkout@v3\n",
    "\n",
    "    - name: Set up Python 3.11\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.11'\n",
    "\n",
    "    - name: Install Dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "    - name: Run Unit Tests\n",
    "      run: |\n",
    "        # Run unit tests\n",
    "        # Assumes pure unit tests are in 'tests/unit/'\n",
    "        python -m unittest discover tests/unit -p \"test_*.py\"\n",
    "\n",
    "    - name: Run Integration Tests\n",
    "      run: |\n",
    "        # Run integration tests\n",
    "        # Assumes integration tests are in 'tests/integration/' and end with '_integration.py'\n",
    "        python -m unittest discover tests/integration -p \"test_*.py\"\n",
    "\n",
    "    - name: Run Functional Tests\n",
    "      run: |\n",
    "        # Run functional tests\n",
    "        # Assumes functional tests are in 'tests/functional/'\n",
    "        python -m unittest discover tests/functional -p \"test_*.py\"\n",
    "\n",
    "    - name: Run E2E Tests\n",
    "      run: |\n",
    "        # Run E2E tests\n",
    "        # Assumes E2E tests are in 'tests/e2e/' and are named e.g., 'e2e_test.py'\n",
    "        python -m unittest discover tests/e2e -p \"test_*.py\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485db47",
   "metadata": {},
   "source": [
    "Here an example of the requirements.txt for this project:\n",
    "\n",
    "```\n",
    "# Core ML Libraries\n",
    "pandas~=2.0.0      # For data manipulation and analysis\n",
    "numpy~=1.24.0      # For numerical operations\n",
    "scikit-learn~=1.2.0 # For machine learning models, preprocessing, and metrics\n",
    "joblib~=1.2.0      # For saving and loading Python objects, especially scikit-learn models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875f479",
   "metadata": {},
   "source": [
    "### Pre-Push CI/CD Checklist\n",
    "\n",
    "Here's a checklist to run through before pushing your changes to the main branch, especially when that push is set to trigger your CI/CD pipeline on GitHub. This helps catch common issues before the automation kicks in.\n",
    "\n",
    "- [ ] Code Clean: All local changes committed, no temporary or debug code.\n",
    "\n",
    "- [ ] Tests Pass: All relevant local tests run successfully.\n",
    "\n",
    "- [ ] Dependencies Current: requirements.txt updated and tested if needed.\n",
    "\n",
    "- [ ] Documentation Updated: README.md reflects any changes in usage or setup.\n",
    "\n",
    "- [ ] Workflow Valid: CI/CD workflow files (.yml) are syntactically correct if modified.\n",
    "\n",
    "- [ ] Clear Commit: Commit message concisely describes the changes.\n",
    "\n",
    "- [ ] Final Review: Briefly checked all changes and confirming push to main.\n",
    "\n",
    "\n",
    "After your checklist is complete:\n",
    "\n",
    "- Prepare Changes: \n",
    "\n",
    "    - `git add .` (stages all your modified files)\n",
    "\n",
    "    - `git commit -m \"Your concise commit message\"` (saves the changes locally with a note)\n",
    "\n",
    "- Send to GitHub:\n",
    "\n",
    "    - `git push origin main` (sends your committed changes to the main branch on GitHub)\n",
    "\n",
    "- On GitHub, immediately after your push:\n",
    "\n",
    "    - Your updated code appears in the repository.\n",
    "\n",
    "    - GitHub Actions detects the change.\n",
    "\n",
    "    - Your CI/CD workflow automatically starts running.\n",
    "\n",
    "    - You can watch its progress and results in the \"Actions\" tab of your GitHub repository.\n",
    "\n",
    "\n",
    "If no errors are detected... \n",
    "\n",
    "__Congratulations, you have deployed an ML CI/CD Process!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c626d",
   "metadata": {},
   "source": [
    "## README.md\n",
    "\n",
    "Serves as the front door to your project. It should be clear, comprehensive, and easy to navigate, allowing anyone (from a new collaborator to a future self) to quickly understand and engage with your work. `A good README.md` is like a clear project instruction manual and storefront. It should quickly tell you:\n",
    "\n",
    "- What it is: A brief project summary and its purpose.\n",
    "\n",
    "- How to set it up: Clear steps to get the code running locally, including dependencies (requirements.txt).\n",
    "\n",
    "- How to use it: Examples and commands to run the main functionalities.\n",
    "\n",
    "- What's inside (for ML): Basic info on the data and model, and key results.\n",
    "\n",
    "- Who made it & how to contribute: Information for collaborators and contact.\n",
    "\n",
    "- Legal bits: The project's license.\n",
    "\n",
    "- Essentially, it answers \"What is this?\", \"How do I get it working?\", and \"How do I use it?\".\n",
    "\n",
    "\n",
    "You can find an example of this file at the end of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224f104",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction ML Pipeline with Comprehensive Testing\n",
    "\n",
    "This project demonstrates a complete, production-style machine learning pipeline for predicting customer churn. Its primary focus is to showcase best practices in MLOps and software engineering, particularly a robust and layered testing strategy using Python's built-in `unittest` framework.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to train a classification model (Logistic Regression) on a customer churn dataset. The pipeline handles data loading, cleaning, feature engineering, model training, evaluation, and artifact logging.\n",
    "\n",
    "The key highlight is the comprehensive test suite, which is structured to reflect the \"Testing Pyramid,\" ensuring code quality, reliability, and maintainability.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "The project is organized into a clear and modular structure:\n",
    "\n",
    "```\n",
    "churn_ml_project_unittest_training1/\n",
    "├── data/\n",
    "│   └── raw/\n",
    "│       └── churn_data.csv      # Raw input data\n",
    "├── model_store/                # Output directory for saved models and logs\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── config.py               # Centralized configuration for the pipeline\n",
    "│   ├── data_loader.py          # Module for loading data\n",
    "│   ├── model.py                # Model class wrapper and metric functions\n",
    "│   ├── preprocessing.py        # Data cleaning and feature engineering functions\n",
    "│   └── pipeline.py             # Main pipeline orchestration logic\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── unit/                   # Unit tests for individual functions\n",
    "│   ├── integration/            # Integration tests for component interactions\n",
    "│   ├── functional/             # Functional tests for user-facing behavior\n",
    "│   └── e2e/                    # End-to-end test for the full pipeline run\n",
    "├── main.py                     # Main entrypoint to run the pipeline\n",
    "└── requirements.txt            # Project dependencies\n",
    "```\n",
    "\n",
    "## Key Concepts Demonstrated\n",
    "\n",
    "*   **Modular Code**: Each part of the ML pipeline (data loading, preprocessing, modeling) is separated into its own module for clarity and reusability.\n",
    "*   **Centralized Configuration**: Key parameters, file paths, and column names are managed in `src/config.py` for easy modification.\n",
    "*   **MLOps Best Practices**: The pipeline saves the trained model and logs key metrics and metadata from each run, which is crucial for experiment tracking and reproducibility.\n",
    "*   **Layered Testing Strategy**: The project implements a full spectrum of tests:\n",
    "    *   **Unit Tests**: Fast tests that verify the smallest pieces of code (individual functions) in isolation.\n",
    "    *   **Integration Tests**: Tests that ensure different components of the pipeline work together correctly (e.g., data loading and cleaning).\n",
    "    *   **Functional Tests**: Tests that verify the application's entrypoint (`main.py`) from a user's perspective, checking its orchestration logic and console output.\n",
    "    *   **End-to-End (E2E) Tests**: A full run of the pipeline on a small, real dataset to ensure the entire system works as a whole and produces the expected final artifacts.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "1.  **Clone the repository:**\n",
    "    ```bash\n",
    "    git clone <repository-url>\n",
    "    cd churn_ml_project_unittest_training1\n",
    "    ```\n",
    "\n",
    "2.  **Create and activate a virtual environment (recommended):**\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
    "    ```\n",
    "\n",
    "3.  **Install the dependencies:**\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "## How to Run\n",
    "\n",
    "### Running the ML Pipeline\n",
    "\n",
    "To execute the entire machine learning pipeline, run the `main.py` script from the project's root directory.\n",
    "\n",
    "```bash\n",
    "python main.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "1.  Load the raw data from `data/raw/churn_data.csv`.\n",
    "2.  Execute the full cleaning, transformation, and training pipeline.\n",
    "3.  Save the trained model to `model_store/churn_model.joblib`.\n",
    "4.  Save the run metrics and metadata to `model_store/log.json`.\n",
    "\n",
    "### Running the Tests\n",
    "\n",
    "The project uses Python's `unittest` framework. You can run all tests using the `discover` command from the root directory.\n",
    "\n",
    "**Run all tests:**\n",
    "```bash\n",
    "python -m unittest discover -s tests -v\n",
    "```\n",
    "\n",
    "**Run a specific type of test:**\n",
    "\n",
    "*   **Unit Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/unit -v\n",
    "    ```\n",
    "*   **Integration Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/integration -v\n",
    "    ```\n",
    "*   **Functional Tests:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/functional -v\n",
    "    ```\n",
    "*   **End-to-End Test:**\n",
    "    ```bash\n",
    "    python -m unittest discover -s tests/e2e -v\n",
    "    ```\n",
    "\n",
    "**Run a specific test file:**\n",
    "```bash\n",
    "python -m unittest tests/unit/test_model.py -v\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This project serves as a robust template for building and validating production-ready machine learning applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae0f81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "752dd116",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf593914",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
